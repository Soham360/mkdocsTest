{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Documentation Dashboard body { font-family: Arial, sans-serif; margin: 20px; max-width: 90%; margin-right: auto; } .dropdown-container { display: flex; gap: 10px; margin: 10px 0; } #repository-dropdown, #file-dropdown { padding: 10px; flex: 1; } #file-content pre { background-color: #f5f5f5; padding: 10px; border: 1px solid #ddd; overflow-x: auto; font-size: 12px; /* Smaller text */ white-space: pre-wrap; /* Wrap long lines */ } Select a Repository CI Components Catalog PPODLinkML basic_skills galyleo hello_icicle_auth_clients Select a YAML file File Content const repoPaths = { \"CI-Components-Catalog\": \"ICICLE-ai/CI-Components-Catalog\", \"PPODLinkML\": \"ICICLE-ai/PPODLinkML\", \"basic_skills\": \"Soham360/basic_skills\", \"galyleo\": \"Soham360/galyleo\", \"hello_icicle_auth_clients\": \"ICICLE-ai/hello_icicle_auth_clients\" // Add GitHub paths for more repositories if needed }; const githubApiUrl = \"https://api.github.com/repos/\"; async function populateFileDropdown() { const repoDropdown = document.getElementById('repository-dropdown'); const fileDropdown = document.getElementById('file-dropdown'); const selectedRepo = repoDropdown.value; // Clear previous options fileDropdown.innerHTML = '<option value=\"\">Select a YAML file</option>'; if (selectedRepo) { const files = await fetchYAMLFiles(selectedRepo); files.forEach(file => { const option = document.createElement('option'); option.value = file; option.textContent = file; fileDropdown.appendChild(option); }); } } async function fetchYAMLFiles(repoName) { const response = await fetch(`${githubApiUrl}${repoPaths[repoName]}/contents/`); const data = await response.json(); let files = []; await Promise.all(data.map(async (item) => { if (item.type === 'dir') { files = files.concat(await fetchFilesRecursively(item.url)); } else if (item.name.endsWith('.yaml') || item.name.endsWith('.yml')) { files.push(item.path); } })); return files; } async function fetchFilesRecursively(dirUrl) { const response = await fetch(dirUrl); const data = await response.json(); let files = []; await Promise.all(data.map(async (item) => { if (item.type === 'dir') { files = files.concat(await fetchFilesRecursively(item.url)); } else if (item.name.endsWith('.yaml') || item.name.endsWith('.yml')) { files.push(item.path); } })); return files; } async function displayFileContent() { const repoDropdown = document.getElementById('repository-dropdown'); const fileDropdown = document.getElementById('file-dropdown'); const fileContentDiv = document.getElementById('file-content'); const selectedRepo = repoDropdown.value; const selectedFile = fileDropdown.value; if (selectedRepo && selectedFile) { const response = await fetch(`https://raw.githubusercontent.com/${repoPaths[selectedRepo]}/master/${selectedFile}`); const content = await response.text(); fileContentDiv.innerHTML = `<pre>${content}</pre>`; } else { fileContentDiv.innerHTML = ''; } }","title":"Documentation Dashboard"},{"location":"#documentation-dashboard","text":"body { font-family: Arial, sans-serif; margin: 20px; max-width: 90%; margin-right: auto; } .dropdown-container { display: flex; gap: 10px; margin: 10px 0; } #repository-dropdown, #file-dropdown { padding: 10px; flex: 1; } #file-content pre { background-color: #f5f5f5; padding: 10px; border: 1px solid #ddd; overflow-x: auto; font-size: 12px; /* Smaller text */ white-space: pre-wrap; /* Wrap long lines */ } Select a Repository CI Components Catalog PPODLinkML basic_skills galyleo hello_icicle_auth_clients Select a YAML file","title":"Documentation Dashboard"},{"location":"#file-content","text":"const repoPaths = { \"CI-Components-Catalog\": \"ICICLE-ai/CI-Components-Catalog\", \"PPODLinkML\": \"ICICLE-ai/PPODLinkML\", \"basic_skills\": \"Soham360/basic_skills\", \"galyleo\": \"Soham360/galyleo\", \"hello_icicle_auth_clients\": \"ICICLE-ai/hello_icicle_auth_clients\" // Add GitHub paths for more repositories if needed }; const githubApiUrl = \"https://api.github.com/repos/\"; async function populateFileDropdown() { const repoDropdown = document.getElementById('repository-dropdown'); const fileDropdown = document.getElementById('file-dropdown'); const selectedRepo = repoDropdown.value; // Clear previous options fileDropdown.innerHTML = '<option value=\"\">Select a YAML file</option>'; if (selectedRepo) { const files = await fetchYAMLFiles(selectedRepo); files.forEach(file => { const option = document.createElement('option'); option.value = file; option.textContent = file; fileDropdown.appendChild(option); }); } } async function fetchYAMLFiles(repoName) { const response = await fetch(`${githubApiUrl}${repoPaths[repoName]}/contents/`); const data = await response.json(); let files = []; await Promise.all(data.map(async (item) => { if (item.type === 'dir') { files = files.concat(await fetchFilesRecursively(item.url)); } else if (item.name.endsWith('.yaml') || item.name.endsWith('.yml')) { files.push(item.path); } })); return files; } async function fetchFilesRecursively(dirUrl) { const response = await fetch(dirUrl); const data = await response.json(); let files = []; await Promise.all(data.map(async (item) => { if (item.type === 'dir') { files = files.concat(await fetchFilesRecursively(item.url)); } else if (item.name.endsWith('.yaml') || item.name.endsWith('.yml')) { files.push(item.path); } })); return files; } async function displayFileContent() { const repoDropdown = document.getElementById('repository-dropdown'); const fileDropdown = document.getElementById('file-dropdown'); const fileContentDiv = document.getElementById('file-content'); const selectedRepo = repoDropdown.value; const selectedFile = fileDropdown.value; if (selectedRepo && selectedFile) { const response = await fetch(`https://raw.githubusercontent.com/${repoPaths[selectedRepo]}/master/${selectedFile}`); const content = await response.text(); fileContentDiv.innerHTML = `<pre>${content}</pre>`; } else { fileContentDiv.innerHTML = ''; } }","title":"File Content"},{"location":"CI-Components-Catalog/","text":"CI Components Catalog Introduction This repository contains metadata and code for the ICICLE CI Components Catalog. The CI Components Catalog tracks all major products developed by the ICICLE AI Institute. Using the Catalog, members within ICICLE as well as their collaborators and the general public can learn about the products being produced. Catalog Schema We are using LinkML and JSONSchema to describe the data model associated with components in the catalog. The JSONSchema document can be generated from the LinkML yaml document by doing the following: $ docker run -v $(pwd):/work -w /work/ --rm -it jstubbs/linkml gen-json-schema ci-component.yaml You can test the schema by validating the local example dataset components-data.yaml , included in the repository, by doing the following: docker run -v $(pwd):/work -w /work/ --rm -it jstubbs/linkml linkml-validate -s ci-component.yaml components-data.yaml If no errors are returned, the message None will be output. Deploying the Catalog Locally A simple prototype application is being developed with Flask and Docker. You need to install Docker on your computer before trying to run the application locally. See the Docker documentation for information about how to get Docker on your platform. You can deploy the prototype locally using one of the following methods: Using Make If you have GNU make on your computer, issuing the following command should build the application image and start the container in one go: make run Building with Docker One can build the container image using a command such as: docker build -t tapis/ci-catalog . With the image build, start the application using: docker run --name catalog --rm -p 5000:5000 tapis/ci-catalog Application URLs With the application deployed locally, navigate to localhost:5000/data to see the catalog overview. Note that the root URL, localhost:5000 , redirects to the catalog overview. The catalog also provides a \"details\" screen for each component. Navigate to localhost:5000/data/<component_id> to see the details of a component. For example, by navigating to http://localhost:5000/data/CameraTrapsEdgeSim , we see details about the Camera Traps project: Authentication with Tapis The CI Components Catalog is designed to use Tapis authentication and authorization. Components in the catalog can be restricted to members of ICICLE by setting publicAccess to false . Access can be further restricted by setting the restrictedToRole attribute on a component -- in this case, a person must be a member of the corresponding Tapis role to view the component. In order to leverage Tapis authn/z in the catalog running locally, you need to configure it with a Tapis OAuth client. Generating an OAuth client can be done following the steps below; see the docs for more details. Create an OAuth Client (One Time Setup) First, generate a JWT and export it to the environment: curl -H \"Content-type: application/json\" -d '{\"username\": \"your_username\", \"password\": \"your_password\", \"grant_type\": \"password\" }' https://icicle.tapis.io/v3/oauth2/tokens export JWT=eyJ0eXAiOiJK.... Use the token to register an OAuth client: curl -H \"X-Tapis-Token: $JWT\" -H \"Content-type: application/json\" -d '{\"client_id\": \"ci-comps-test\", \"callback_url\": \"http://localhost:5000/oauth2/callback\", \"client_key\": \"icicle4ever\"}' https://icicle.tapis.io/v3/oauth2/clients Configure the Catalog to Use the Client The catalog needs to be configured with the client id and key to work with Tapis OAuth. Simply add the id and key to the config.yaml file. Deployment to Pods A hosted version of the Catalog is now running on the Tapis Pods API. Navigate to the Production URL to interact with the most recently deployed version. Update the Deployment NOTE: Updating the deployment requires access to the tapis organization on Docker Hub to be able to push a new version of the image as well as access to the components pod in the ICICLE tenant. Reach out on the #icicle_ci_component_catalog channel of tacc-cloud.slack.com if you need help getting access. Updating the deployment on Tapis Pods is incredibly easy -- just do the following: 1) Push a new version of the image to docker hub: tapis/ci-catalog:latest 2) Use the restart operation to restart the pod; this will pull the new image: curl -H \"x-tapis-token: $token\" https://icicle.tapis.io/v3/pods/components/restart You can check on the deployment by doing: curl -H \"x-tapis-token: $token\" https://icicle.tapis.io/v3/pods/components/catalog And you can see recent logs of the deployed version with: curl -H \"x-tapis-token: $token\" https://icicle.tapis.io/v3/pods/components/logs Creating the Initial Deployment NOTE: Registering the same pod again is not only unnecessary but it will not work, because the id is already taken. We leave this here only to document what was done, for posterity. The initial deployment to the pods service involved creating a new pod with the \"components\" ID. We used the following curl command to register the pod. curl -H \"x-tapis-token: $token\" https://icicle.tapis.io/v3/pods -H \"content-type: application/json\" -d '{\"pod_id\": \"components\", \"pod_template\": \"tapis/ci-catalog\", \"description\": \"Pod for hosted version of the ICICLE CI Component Catalog\", \"environment_variables\": {\"client_id\": \"<the prod client id>\", \"client_key\": \"<the prod client key>\",\"app_base_url\": \"https://components.pods.icicle.tapis.io\"}, \"time_to_stop_instance\": -1}' Here, $token must be a valid Tapis JWT in the icicle tenant for a user authorized for the components pod. Acknowledgements This work has been funded by grants from the National Science Foundation, including the ICICLE AI Institute (OAC 2112606)","title":"CI Components Catalog"},{"location":"CI-Components-Catalog/#ci-components-catalog","text":"","title":"CI Components Catalog"},{"location":"CI-Components-Catalog/#introduction","text":"This repository contains metadata and code for the ICICLE CI Components Catalog. The CI Components Catalog tracks all major products developed by the ICICLE AI Institute. Using the Catalog, members within ICICLE as well as their collaborators and the general public can learn about the products being produced.","title":"Introduction"},{"location":"CI-Components-Catalog/#catalog-schema","text":"We are using LinkML and JSONSchema to describe the data model associated with components in the catalog. The JSONSchema document can be generated from the LinkML yaml document by doing the following: $ docker run -v $(pwd):/work -w /work/ --rm -it jstubbs/linkml gen-json-schema ci-component.yaml You can test the schema by validating the local example dataset components-data.yaml , included in the repository, by doing the following: docker run -v $(pwd):/work -w /work/ --rm -it jstubbs/linkml linkml-validate -s ci-component.yaml components-data.yaml If no errors are returned, the message None will be output.","title":"Catalog Schema"},{"location":"CI-Components-Catalog/#deploying-the-catalog-locally","text":"A simple prototype application is being developed with Flask and Docker. You need to install Docker on your computer before trying to run the application locally. See the Docker documentation for information about how to get Docker on your platform. You can deploy the prototype locally using one of the following methods:","title":"Deploying the Catalog Locally"},{"location":"CI-Components-Catalog/#using-make","text":"If you have GNU make on your computer, issuing the following command should build the application image and start the container in one go: make run","title":"Using Make"},{"location":"CI-Components-Catalog/#building-with-docker","text":"One can build the container image using a command such as: docker build -t tapis/ci-catalog . With the image build, start the application using: docker run --name catalog --rm -p 5000:5000 tapis/ci-catalog","title":"Building with Docker"},{"location":"CI-Components-Catalog/#application-urls","text":"With the application deployed locally, navigate to localhost:5000/data to see the catalog overview. Note that the root URL, localhost:5000 , redirects to the catalog overview. The catalog also provides a \"details\" screen for each component. Navigate to localhost:5000/data/<component_id> to see the details of a component. For example, by navigating to http://localhost:5000/data/CameraTrapsEdgeSim , we see details about the Camera Traps project:","title":"Application URLs"},{"location":"CI-Components-Catalog/#authentication-with-tapis","text":"The CI Components Catalog is designed to use Tapis authentication and authorization. Components in the catalog can be restricted to members of ICICLE by setting publicAccess to false . Access can be further restricted by setting the restrictedToRole attribute on a component -- in this case, a person must be a member of the corresponding Tapis role to view the component. In order to leverage Tapis authn/z in the catalog running locally, you need to configure it with a Tapis OAuth client. Generating an OAuth client can be done following the steps below; see the docs for more details.","title":"Authentication with Tapis"},{"location":"CI-Components-Catalog/#create-an-oauth-client-one-time-setup","text":"First, generate a JWT and export it to the environment: curl -H \"Content-type: application/json\" -d '{\"username\": \"your_username\", \"password\": \"your_password\", \"grant_type\": \"password\" }' https://icicle.tapis.io/v3/oauth2/tokens export JWT=eyJ0eXAiOiJK.... Use the token to register an OAuth client: curl -H \"X-Tapis-Token: $JWT\" -H \"Content-type: application/json\" -d '{\"client_id\": \"ci-comps-test\", \"callback_url\": \"http://localhost:5000/oauth2/callback\", \"client_key\": \"icicle4ever\"}' https://icicle.tapis.io/v3/oauth2/clients","title":"Create an OAuth Client (One Time Setup)"},{"location":"CI-Components-Catalog/#configure-the-catalog-to-use-the-client","text":"The catalog needs to be configured with the client id and key to work with Tapis OAuth. Simply add the id and key to the config.yaml file.","title":"Configure the Catalog to Use the Client"},{"location":"CI-Components-Catalog/#deployment-to-pods","text":"A hosted version of the Catalog is now running on the Tapis Pods API. Navigate to the Production URL to interact with the most recently deployed version.","title":"Deployment to Pods"},{"location":"CI-Components-Catalog/#update-the-deployment","text":"NOTE: Updating the deployment requires access to the tapis organization on Docker Hub to be able to push a new version of the image as well as access to the components pod in the ICICLE tenant. Reach out on the #icicle_ci_component_catalog channel of tacc-cloud.slack.com if you need help getting access. Updating the deployment on Tapis Pods is incredibly easy -- just do the following: 1) Push a new version of the image to docker hub: tapis/ci-catalog:latest 2) Use the restart operation to restart the pod; this will pull the new image: curl -H \"x-tapis-token: $token\" https://icicle.tapis.io/v3/pods/components/restart You can check on the deployment by doing: curl -H \"x-tapis-token: $token\" https://icicle.tapis.io/v3/pods/components/catalog And you can see recent logs of the deployed version with: curl -H \"x-tapis-token: $token\" https://icicle.tapis.io/v3/pods/components/logs","title":"Update the Deployment"},{"location":"CI-Components-Catalog/#creating-the-initial-deployment","text":"NOTE: Registering the same pod again is not only unnecessary but it will not work, because the id is already taken. We leave this here only to document what was done, for posterity. The initial deployment to the pods service involved creating a new pod with the \"components\" ID. We used the following curl command to register the pod. curl -H \"x-tapis-token: $token\" https://icicle.tapis.io/v3/pods -H \"content-type: application/json\" -d '{\"pod_id\": \"components\", \"pod_template\": \"tapis/ci-catalog\", \"description\": \"Pod for hosted version of the ICICLE CI Component Catalog\", \"environment_variables\": {\"client_id\": \"<the prod client id>\", \"client_key\": \"<the prod client key>\",\"app_base_url\": \"https://components.pods.icicle.tapis.io\"}, \"time_to_stop_instance\": -1}' Here, $token must be a valid Tapis JWT in the icicle tenant for a user authorized for the components pod.","title":"Creating the Initial Deployment"},{"location":"CI-Components-Catalog/#acknowledgements","text":"This work has been funded by grants from the National Science Foundation, including the ICICLE AI Institute (OAC 2112606)","title":"Acknowledgements"},{"location":"CI-Components-Catalog/CHANGELOG/","text":"Change Log All notable changes to this project will be documented in this file. 0.1.0 - 2023-04-20 This is the initial public release of the ICICLE CI Components Catlog, a web-based application that tracks all major products developed by the ICICLE AI Institute. Using the Catalog, members within ICICLE as well as their collaborators and the general public can learn about the products being produced. Breaking Changes: None. New features: This initial release provides version 0.1.0 of the components schema in LinkML and JSONSchema format. Component data are stored in a flat file and served via a Flask application on hosted on the Tapis Pods service at https://components.pods.icicle.tapis.io/. This initial version supports private components, restricted to either ICICLE members of specific groups of members; this functionality utilizes authentication and authorization based on Tapis. Bug fixes: None.","title":"Change Log"},{"location":"CI-Components-Catalog/CHANGELOG/#change-log","text":"All notable changes to this project will be documented in this file.","title":"Change Log"},{"location":"CI-Components-Catalog/CHANGELOG/#010-2023-04-20","text":"This is the initial public release of the ICICLE CI Components Catlog, a web-based application that tracks all major products developed by the ICICLE AI Institute. Using the Catalog, members within ICICLE as well as their collaborators and the general public can learn about the products being produced.","title":"0.1.0 - 2023-04-20"},{"location":"CI-Components-Catalog/CHANGELOG/#breaking-changes","text":"None.","title":"Breaking Changes:"},{"location":"CI-Components-Catalog/CHANGELOG/#new-features","text":"This initial release provides version 0.1.0 of the components schema in LinkML and JSONSchema format. Component data are stored in a flat file and served via a Flask application on hosted on the Tapis Pods service at https://components.pods.icicle.tapis.io/. This initial version supports private components, restricted to either ICICLE members of specific groups of members; this functionality utilizes authentication and authorization based on Tapis.","title":"New features:"},{"location":"CI-Components-Catalog/CHANGELOG/#bug-fixes","text":"None.","title":"Bug fixes:"},{"location":"CI-Components-Catalog/architecture/","text":"ICICLE Architecture Conceptual Components This directory contains a list of ICICLE conceptual components for building architecture diagrams. It contains the following files: architecture-component-schema.yaml -- The schema governing the structure of the components. common_arch_v1.yml -- The actual components. Validate the Architecture Components docker run -v $(pwd):/work -w /work/ --rm -it jstubbs/linkml linkml-validate -s architecture/architecture-component-schema.yaml architecture/common_arch_v1.yml Visualization in Neo4j You can visualize the architecture data using the hosted Neo4j instance serving the most recent version of the data. The connection information is as follows: URL: bolt+s://cicatalog.pods.icicle.tapis.io Port: 443 User: cicatalog Password: Ask on the ICICLE Slack channel. The data can be visualized in a browser using the https://browser.neo4j.io/ website. Common Queries When working with the data, the following queries may be helpful: Select all nodes: match (n) return n Select the nodes for a specific project: MATCH (n) WHERE n.primaryThrust = 'useInspired/DA' RETURN n Select all nodes from two projects: MATCH (n) WHERE (n.primaryThrust = 'useInspired/DA') OR (n.primaryThrust = 'useInspired/AE') RETURN n Select the nodes for a project or nodes that are common (i.e., not part of a specific project) MATCH (n) WHERE (n.primaryThrust = 'useInspired/DA') OR (NOT EXISTS(n.primaryThrust)) RETURN n Provide the list of components to pull in, by id: MATCH (n) WHERE (n.id = 'mc') OR (n.id = 'ui') OR (n.id = 'int_plane') OR (n.id = 'authn_z') OR (n.id = 'ml_edge_to_center_middleware') RETURN n Updating the Data (Admins) We have automated the ETL pipeline that updates the Neo4j instance above from the current dataset in the git repository. It is a Tapis workflow and can be launched from the Tapis UI: Log in to the Tapis UI with your TACC account: https://icicle.tapis.io/tapis-ui/# Navigate to the pipelines: https://icicle.tapis.io/tapis-ui/#/workflows/pipelines/IKLE/ Click Run for the reference_architecture_pipeline","title":"ICICLE Architecture Conceptual Components"},{"location":"CI-Components-Catalog/architecture/#icicle-architecture-conceptual-components","text":"This directory contains a list of ICICLE conceptual components for building architecture diagrams. It contains the following files: architecture-component-schema.yaml -- The schema governing the structure of the components. common_arch_v1.yml -- The actual components.","title":"ICICLE Architecture Conceptual Components"},{"location":"CI-Components-Catalog/architecture/#validate-the-architecture-components","text":"docker run -v $(pwd):/work -w /work/ --rm -it jstubbs/linkml linkml-validate -s architecture/architecture-component-schema.yaml architecture/common_arch_v1.yml","title":"Validate the Architecture Components"},{"location":"CI-Components-Catalog/architecture/#visualization-in-neo4j","text":"You can visualize the architecture data using the hosted Neo4j instance serving the most recent version of the data. The connection information is as follows: URL: bolt+s://cicatalog.pods.icicle.tapis.io Port: 443 User: cicatalog Password: Ask on the ICICLE Slack channel. The data can be visualized in a browser using the https://browser.neo4j.io/ website.","title":"Visualization in Neo4j"},{"location":"CI-Components-Catalog/architecture/#common-queries","text":"When working with the data, the following queries may be helpful: Select all nodes: match (n) return n Select the nodes for a specific project: MATCH (n) WHERE n.primaryThrust = 'useInspired/DA' RETURN n Select all nodes from two projects: MATCH (n) WHERE (n.primaryThrust = 'useInspired/DA') OR (n.primaryThrust = 'useInspired/AE') RETURN n Select the nodes for a project or nodes that are common (i.e., not part of a specific project) MATCH (n) WHERE (n.primaryThrust = 'useInspired/DA') OR (NOT EXISTS(n.primaryThrust)) RETURN n Provide the list of components to pull in, by id: MATCH (n) WHERE (n.id = 'mc') OR (n.id = 'ui') OR (n.id = 'int_plane') OR (n.id = 'authn_z') OR (n.id = 'ml_edge_to_center_middleware') RETURN n","title":"Common Queries"},{"location":"CI-Components-Catalog/architecture/#updating-the-data-admins","text":"We have automated the ETL pipeline that updates the Neo4j instance above from the current dataset in the git repository. It is a Tapis workflow and can be launched from the Tapis UI: Log in to the Tapis UI with your TACC account: https://icicle.tapis.io/tapis-ui/# Navigate to the pipelines: https://icicle.tapis.io/tapis-ui/#/workflows/pipelines/IKLE/ Click Run for the reference_architecture_pipeline","title":"Updating the Data (Admins)"},{"location":"PPODLinkML/","text":"PPOD_FSL - A LinkML Schema for UC Davis Food Systems Lab Applications This repository contains a LinkML schema for a version of the PPOD (Persons-Projects-Organizations-Datasets) data pattern that describes resources being cataloged by the UC Davis Food Systems Lab . These are resources pertinent to California foodsheds and conservation activities, and include lists of organizations, people, programs, projects, tools, datasets, and guidelines and mandates. These resources are currently maintained in a Google Sheets document which is converted into a RDF Turtle file using a Python script that is posted in the PPODtottl repository . LinkML is an emerging standard and toolset for describing data schemas with an orientation towards building linked data applications. LinkML data schemas are written in YAML and the framework provides tools to convert these schemas into a number of other formats, including JSON-Schema, OWL, SQL DDL, SHACL, ShEx, and Python classes. The framework also provides tools for validating and converting data between different formats including RDF, CSV, JSON, YAML, and SQLite databases. As such LinkML is intended as a lingua franca for interoperability between data schemas and datasets. This repository contains two main files, PPOD.yaml and vocabs.yaml . PPOD.yaml contains the classes and slots describing the resources in the Google Sheets document and the RDF file that it generates, and vocabs.yaml gives the enumerated vocabularies that are being used. These enumerated vocabularies include categories such as types of organizations and lists of sustainability issues, ecoregions, and habitat types. LinkML provides a facility to automatically generate a set of web pages describing the elements in the schema, one web page per element. This has been carried out for the PPOD FSL schema and has been posted on GitHub Pages . Further work on PPOD_FSL will include providing standardized identifiers for newly created data schema elements and instance data using the URL resolution service w3id.org . Over the near future, work on PPOD schemas will shift to modularizing PPOD into a PPOD core and individual PPOD application schemas that inherit classes and slots from PPOD-core. Acknowledgements Work on the PPOD FSL LinkML schema has been funded under the NSF ICICLE AI Institute , grant number OAC-2112606.","title":"PPOD_FSL - A LinkML Schema for UC Davis Food Systems Lab Applications"},{"location":"PPODLinkML/#ppod_fsl-a-linkml-schema-for-uc-davis-food-systems-lab-applications","text":"This repository contains a LinkML schema for a version of the PPOD (Persons-Projects-Organizations-Datasets) data pattern that describes resources being cataloged by the UC Davis Food Systems Lab . These are resources pertinent to California foodsheds and conservation activities, and include lists of organizations, people, programs, projects, tools, datasets, and guidelines and mandates. These resources are currently maintained in a Google Sheets document which is converted into a RDF Turtle file using a Python script that is posted in the PPODtottl repository . LinkML is an emerging standard and toolset for describing data schemas with an orientation towards building linked data applications. LinkML data schemas are written in YAML and the framework provides tools to convert these schemas into a number of other formats, including JSON-Schema, OWL, SQL DDL, SHACL, ShEx, and Python classes. The framework also provides tools for validating and converting data between different formats including RDF, CSV, JSON, YAML, and SQLite databases. As such LinkML is intended as a lingua franca for interoperability between data schemas and datasets. This repository contains two main files, PPOD.yaml and vocabs.yaml . PPOD.yaml contains the classes and slots describing the resources in the Google Sheets document and the RDF file that it generates, and vocabs.yaml gives the enumerated vocabularies that are being used. These enumerated vocabularies include categories such as types of organizations and lists of sustainability issues, ecoregions, and habitat types. LinkML provides a facility to automatically generate a set of web pages describing the elements in the schema, one web page per element. This has been carried out for the PPOD FSL schema and has been posted on GitHub Pages . Further work on PPOD_FSL will include providing standardized identifiers for newly created data schema elements and instance data using the URL resolution service w3id.org . Over the near future, work on PPOD schemas will shift to modularizing PPOD into a PPOD core and individual PPOD application schemas that inherit classes and slots from PPOD-core.","title":"PPOD_FSL - A LinkML Schema for UC Davis Food Systems Lab Applications"},{"location":"PPODLinkML/#acknowledgements","text":"Work on the PPOD FSL LinkML schema has been funded under the NSF ICICLE AI Institute , grant number OAC-2112606.","title":"Acknowledgements"},{"location":"PPODLinkML/LICENSE/","text":"Creative Commons Legal Code CC0 1.0 Universal CREATIVE COMMONS CORPORATION IS NOT A LAW FIRM AND DOES NOT PROVIDE LEGAL SERVICES. DISTRIBUTION OF THIS DOCUMENT DOES NOT CREATE AN ATTORNEY-CLIENT RELATIONSHIP. CREATIVE COMMONS PROVIDES THIS INFORMATION ON AN \"AS-IS\" BASIS. CREATIVE COMMONS MAKES NO WARRANTIES REGARDING THE USE OF THIS DOCUMENT OR THE INFORMATION OR WORKS PROVIDED HEREUNDER, AND DISCLAIMS LIABILITY FOR DAMAGES RESULTING FROM THE USE OF THIS DOCUMENT OR THE INFORMATION OR WORKS PROVIDED HEREUNDER. Statement of Purpose The laws of most jurisdictions throughout the world automatically confer exclusive Copyright and Related Rights (defined below) upon the creator and subsequent owner(s) (each and all, an \"owner\") of an original work of authorship and/or a database (each, a \"Work\"). Certain owners wish to permanently relinquish those rights to a Work for the purpose of contributing to a commons of creative, cultural and scientific works (\"Commons\") that the public can reliably and without fear of later claims of infringement build upon, modify, incorporate in other works, reuse and redistribute as freely as possible in any form whatsoever and for any purposes, including without limitation commercial purposes. These owners may contribute to the Commons to promote the ideal of a free culture and the further production of creative, cultural and scientific works, or to gain reputation or greater distribution for their Work in part through the use and efforts of others. For these and/or other purposes and motivations, and without any expectation of additional consideration or compensation, the person associating CC0 with a Work (the \"Affirmer\"), to the extent that he or she is an owner of Copyright and Related Rights in the Work, voluntarily elects to apply CC0 to the Work and publicly distribute the Work under its terms, with knowledge of his or her Copyright and Related Rights in the Work and the meaning and intended legal effect of CC0 on those rights. Copyright and Related Rights. A Work made available under CC0 may be protected by copyright and related or neighboring rights (\"Copyright and Related Rights\"). Copyright and Related Rights include, but are not limited to, the following: i. the right to reproduce, adapt, distribute, perform, display, communicate, and translate a Work; ii. moral rights retained by the original author(s) and/or performer(s); iii. publicity and privacy rights pertaining to a person's image or likeness depicted in a Work; iv. rights protecting against unfair competition in regards to a Work, subject to the limitations in paragraph 4(a), below; v. rights protecting the extraction, dissemination, use and reuse of data in a Work; vi. database rights (such as those arising under Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, and under any national implementation thereof, including any amended or successor version of such directive); and vii. other similar, equivalent or corresponding rights throughout the world based on applicable law or treaty, and any national implementations thereof. Waiver. To the greatest extent permitted by, but not in contravention of, applicable law, Affirmer hereby overtly, fully, permanently, irrevocably and unconditionally waives, abandons, and surrenders all of Affirmer's Copyright and Related Rights and associated claims and causes of action, whether now known or unknown (including existing as well as future claims and causes of action), in the Work (i) in all territories worldwide, (ii) for the maximum duration provided by applicable law or treaty (including future time extensions), (iii) in any current or future medium and for any number of copies, and (iv) for any purpose whatsoever, including without limitation commercial, advertising or promotional purposes (the \"Waiver\"). Affirmer makes the Waiver for the benefit of each member of the public at large and to the detriment of Affirmer's heirs and successors, fully intending that such Waiver shall not be subject to revocation, rescission, cancellation, termination, or any other legal or equitable action to disrupt the quiet enjoyment of the Work by the public as contemplated by Affirmer's express Statement of Purpose. Public License Fallback. Should any part of the Waiver for any reason be judged legally invalid or ineffective under applicable law, then the Waiver shall be preserved to the maximum extent permitted taking into account Affirmer's express Statement of Purpose. In addition, to the extent the Waiver is so judged Affirmer hereby grants to each affected person a royalty-free, non transferable, non sublicensable, non exclusive, irrevocable and unconditional license to exercise Affirmer's Copyright and Related Rights in the Work (i) in all territories worldwide, (ii) for the maximum duration provided by applicable law or treaty (including future time extensions), (iii) in any current or future medium and for any number of copies, and (iv) for any purpose whatsoever, including without limitation commercial, advertising or promotional purposes (the \"License\"). The License shall be deemed effective as of the date CC0 was applied by Affirmer to the Work. Should any part of the License for any reason be judged legally invalid or ineffective under applicable law, such partial invalidity or ineffectiveness shall not invalidate the remainder of the License, and in such case Affirmer hereby affirms that he or she will not (i) exercise any of his or her remaining Copyright and Related Rights in the Work or (ii) assert any associated claims and causes of action with respect to the Work, in either case contrary to Affirmer's express Statement of Purpose. Limitations and Disclaimers. a. No trademark or patent rights held by Affirmer are waived, abandoned, surrendered, licensed or otherwise affected by this document. b. Affirmer offers the Work as-is and makes no representations or warranties of any kind concerning the Work, express, implied, statutory or otherwise, including without limitation warranties of title, merchantability, fitness for a particular purpose, non infringement, or the absence of latent or other defects, accuracy, or the present or absence of errors, whether or not discoverable, all to the greatest extent permissible under applicable law. c. Affirmer disclaims responsibility for clearing rights of other persons that may apply to the Work or any use thereof, including without limitation any person's Copyright and Related Rights in the Work. Further, Affirmer disclaims responsibility for obtaining any necessary consents, permissions or other rights required for any use of the Work. d. Affirmer understands and acknowledges that Creative Commons is not a party to this document and has no duty or obligation with respect to this CC0 or use of the Work.","title":"LICENSE"},{"location":"basic_skills/","text":"HPC Basic Skills This repository hosts all material for the HPC Training Basic Skill Training Series Last update: January 18, 2024 For information about accessing SDSC HPC systems securely see the SDSC HPC Security Repo License All the teaching material in this repository is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License","title":"HPC Basic Skills"},{"location":"basic_skills/#hpc-basic-skills","text":"This repository hosts all material for the HPC Training Basic Skill Training Series","title":"HPC Basic Skills"},{"location":"basic_skills/#last-update-january-18-2024","text":"","title":"Last update: January 18, 2024"},{"location":"basic_skills/#for-information-about-accessing-sdsc-hpc-systems-securely-see-the-sdsc-hpc-security-repo","text":"","title":"For information about accessing SDSC HPC systems securely see the SDSC HPC Security Repo"},{"location":"basic_skills/#license","text":"All the teaching material in this repository is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License","title":"License"},{"location":"basic_skills/basic_linux_skills_expanse/","text":"Using SDSC HPC Resources: Basic Linux Skills on Expanse When you log on to Expanse , your computer operating system will be a Linux or Unix shell. \\\" A Unix shell is a command-line interpreter or shell that provides a traditional Unix-like command line user interface. This environment is very different from the easy to use GUI interfaces we have all become used in the Windows and MacOS systems \\\" (https://en.wikipedia.org/wiki/Unix_shell). Basic Linux skills are necessary to complete the hands-on exercises. If it\u2019s been a while since you\u2019ve worked in a Linux environment, be sure to reacquaint yourself with the following topic (many of which are demonstrated below)s: copying, listing, deleting and renaming files; using wildcards; navigating directories; changing file permissions; setting environment variables; using common utilities (grep, cat, less, head, sort, tar, gzip). This tutorial is focussed on using Linux on the SDSC Expanse supercomputer. There a a lot of other tutorial on the Web: * A nice tutorial can be found here http://www.ee.surrey.ac.uk/Teaching/Unix/. You should also be comfortable with one of the standard Linux editors, such as vim, emacs, or nano. * For a fun tutorial on Linux, see the UCSD/SDSC Supercomputing Club's tutorial, based on a Google community project, here: https://supercomputing-club.sdsc.edu/posts/advent-of-scc-2023/advent-of-scc-epilogue/ Notes: * For the examples below, we are using the bash shell, which is the default shell for new accounts on Expanse. For the purposes of following SDSC tutorials and exercises, please do not change the shell. * For any Linux command, you can find out what it does by asking for help. The syntax is usually * command --help * man command --> invokes a user guide or manual * search the web using google or some other search engine * In Linux/Unix, everything is a file: a text file, a directory, even the output for a command * Linux/Unix is case sensitive. Examples:: * Basic Environment * Directories and Navigation * Copying directories * Files * Permissions * Wildcards * Common Utilities Note: if you have difficulties completing this task, please contact staff at consult@sdsc.edu . Basic Environment Using Unix commands, we can learn a lot about the machine we are logged onto. Some of the commands are simple: [username@login02 ~]$ date Tue Jan 16 20:20:23 PST 2024 [username@login02 ~]$ hostname login02 [username@login02 ~]$ whoami username Note: To learn about most unix commands, try accessing the man pages: [username@login02 ~]$ man date NAME date - print or set the system date and time SYNOPSIS date [OPTION]... [+FORMAT] date [-u|--utc|--universal] [MMDDhhmm[[CC]YY][.ss]] DESCRIPTION Display the current time in the given FORMAT, or set the system date. ..... more info ..... The unix command env will print out the environment settings for your login session. The list below is an edited summary of all the information Warning: the output can be very long (over 90 lines) [username@login02 ~]$ env MODULEPATH=/opt/modulefiles/mpi/.intel:/opt/modulefiles/applications/.intel:/opt/modulefiles/mpi:/opt/modulefiles/compilers:/opt/modulefiles/applications:/usr/share/Modules/modulefiles:/etc/modulefiles LOADEDMODULES=intel/2013_sp1.2.144:mvapich2_ib/2.1:gnutools/2.69 HOME=/home/username SDSCHOME=/opt/sdsc LOGNAME=username SSH_CONNECTION=xxx.xxx.xx.xx 53640 198.202.113.253 22 DISPLAY=localhost:48.0 It is often useful to print out (or use) environment variables. To print them out, use the echo command, and $ sign (which extracts the value of the shell variable): [username@login02 ~]$ echo $SHELL /bin/bash [username@login02 ~]$ echo $HOME /home/username Another important environment variable is the home directory variable, the \\\"tilde\\\" character: ~ [username@login02 ~]$ echo ~ /home/username [username@login02 ~]$ You can create your own environment variables: [username@login02 ~]$ MY_NAME=\"Super User\" [username@login02 ~]$ echo $MY_NAME Super User Unix has the concept of users and groups. Groups are used to control access to resources (files, applications, etc.) and help establish a secure envionment Users can be in more than one group. To see which groups you are a member of, use the group command: [username@login02 OPENMP]$ groups abc123 pet heart scicom-docs grdclus webwrt scwpf ... Back to Top Directories and Navigation In unix, everything is a file, which can be confusing at first. The locations for where files are stored are called directories (which is equivalent to folders), and are also viewed as files by the operating system. To find out where you are in the system, use the pwd command (print working directory), which prints the full path to the directory you are currently in: [username@login02 ~]$ pwd /home/username To see what are the contents of the current directory are, use the file listing command ls [username@login02 ~]$ ls filelisting.txt intel loadgccomgnuenv.sh loadgnuenv.sh loadintelenv.sh tools In every Unix directory, there are \\\"hidden\\\" files (just like on Macs and Windows machines), to see them, run the ls -a command: [username@login02 ~]$ ls -a . .bash_history .bashrc .gitconfig loadgccomgnuenv.sh .ncviewrc .ssh .vimrc .. .bash_logout .config filelisting.txt intel loadgnuenv.sh .petscconfig tools .Xauthority .alias .bash_profile .kshrc loadintelenv.sh .slurm .viminfo In Unix, sometimes it is hard tell if a file is a directory. To see file details (including timestamp and size), run the ls -l command: [username@login02 ~]$ ls -l -rw-r--r-- 1 username abc123 322 Jul 17 21:04 filelisting.txt drwxr-xr-x 3 username abc123 3 Jun 22 2023 intel -rwx------ 1 username abc123 101 Jun 27 2023 loadgccomgnuenv.sh -rwx------ 1 username abc123 77 Oct 16 2023 loadgnuenv.sh -rwxr-xr-x 1 username abc123 125 Oct 16 2023 loadintelenv.sh drwxr-xr-x 2 username abc123 4 Jun 30 2023 tools You can combine the two commands above and use it to see the full directory and file information: [username@login02 ~]$ ls -al total 166 drwx------ 7 username abc123 23 Jul 17 19:33 . drwxr-xr-x 143 root root 0 Jul 17 20:01 .. -rw-r--r-- 1 username abc123 2487 Jun 23 2023 .alias -rw------- 1 username abc123 14247 Jul 17 12:11 .bash_history -rw-r--r-- 1 username abc123 18 Jun 19 2023 .bash_logout -rw-r--r-- 1 username abc123 176 Jun 19 2023 .bash_profile -rw-r--r-- 1 username abc123 159 Jul 17 18:24 .bashrc drwx------ 3 username abc123 3 Oct 23 2023 .config -rw-r--r-- 1 username abc123 322 Jul 17 21:04 filelisting.txt -rw-r--r-- 1 username abc123 1641 Jun 22 2023 .gccomrc -rw-r--r-- 1 username abc123 245 Jun 28 2023 .gitconfig drwxr-xr-x 3 username abc123 3 Jun 22 2023 intel -rw-r--r-- 1 username abc123 171 Jun 19 2023 .kshrc -rwx------ 1 username abc123 101 Jun 27 2023 loadgccomgnuenv.sh -rwx------ 1 username abc123 77 Oct 16 2023 loadgnuenv.sh -rwxr-xr-x 1 username abc123 125 Oct 16 2023 loadintelenv.sh [snip extra lines] There are several things to notice in the above listing: the first column of data is information about the file \"permissions\\\", which controls who can see/read/modify what files ( r =read, w =write, x =execute, - =no permission); the next 2 columns are the username and groupID; the 3rd and 4th columns are the size and date. This is discussed in more detail in the Permissions section below. Also, note that two files have dots for their names: in unix the \"dot\" is a component of a filename. When working with filenames, a leading dot is the prefix of a \"hidden\" file, a file that an ls will not normally show. But also, the single dot, . represents the current working directory, and the double dots, .. represent the directory above. You use these as arguments to unix commands dealing with directories. There are simple Linux commands to create and remove directories, and to populate the directories. To create a directory, use the mkdir , make directory command (more about directories in the sections below): [username@login02 ~]$ mkdir testdir [username@login02 ~]$ ls -l total 12 drwxr-xr-x 3 username abc123 3 Jun 22 2023 intel -rwx------ 1 username abc123 101 Jun 27 2023 loadgccomgnuenv.sh -rwx------ 1 username abc123 77 Oct 16 2023 loadgnuenv.sh -rwxr-xr-x 1 username abc123 125 Oct 16 2023 loadintelenv.sh drwxr-xr-x 2 username abc123 2 Jul 17 20:49 testdir drwxr-xr-x 2 username abc123 4 Jun 30 2023 tools To move into that directory, use the cd , change directory command: [username@login02 ~]$ cd testdir/ [username@login02 testdir]$ ls -al total 20 drwxr-xr-x 2 username abc123 2 Jul 17 20:49 . drwxr-x--- 9 username abc123 25 Jul 17 20:49 .. [username@login02 testdir]$ From this directory, you can use the .. command to see the contents of the directory above: [username@login02 testdir]$ ls -l .. [username@login02 testdir]$ /bin/ls -l .. total 22 drwxr-xr-x 4 username abc123 5 Jul 17 20:43 expanse-examples -rw-r--r-- 1 username abc123 322 Jul 17 21:04 filelisting.txt drwxr-xr-x 3 username abc123 3 Jun 22 2023 intel -rwx------ 1 username abc123 101 Jun 27 2023 loadgccomgnuenv.sh -rwx------ 1 username abc123 77 Oct 16 2023 loadgnuenv.sh -rwxr-xr-x 1 username abc123 125 Oct 16 2023 loadintelenv.sh drwxr-xr-x 2 username abc123 4 Jul 17 20:53 testdir drwxr-xr-x 2 username abc123 4 Jun 30 2023 tools To remove files and directories there are different mechanisms, depending on whether or not the directory is empty or contains files. For this example, we create a new directory, testdir2 , we'll create three directories using the mkdir command, populate some testfiles using the touch command, and then try to delete the directories using either the rmdir or rm commands. [username@login01 testdir]$cd .. [username@login01 testdir]$ mkdir testdir2 [username@login01 testdir]$ cd testdir2 [username@login01 testdir2]$ mkdir dir1 [username@login01 testdir2]$ mkdir dir2 [username@login01 testdir2]$ mkdir dir3 [username@login01 testdir2]$ ll total 71 drwxr-xr-x 5 username use300 5 Jan 17 22:11 . drwxr-x--- 50 username use300 91 Jan 17 22:01 .. drwxr-xr-x 2 username use300 2 Jan 17 22:11 dir1 drwxr-xr-x 2 username use300 2 Jan 17 22:11 dir2 drwxr-xr-x 2 username use300 2 Jan 17 22:11 dir3 Next, create some testfiles using the touch command: [username@login01 testdir2]$ touch f1 [username@login01 testdir2]$ touch f2 [username@login01 testdir2]$ touch f3 [username@login01 testdir2]$ touch dir2/file1 [username@login01 testdir2]$ touch dir2/file2 [username@login01 testdir2]$ touch dir2/file3 [username@login01 testdir2]$ ls -al total 90 drwxr-xr-x 5 username use300 8 Jan 17 22:23 . drwxr-x--- 50 username use300 91 Jan 17 22:01 .. drwxr-xr-x 2 username use300 5 Jan 17 22:23 dir1 drwxr-xr-x 2 username use300 5 Jan 17 22:17 dir2 drwxr-xr-x 2 username use300 5 Jan 17 22:18 dir3 -rw-r--r-- 1 username use300 0 Jan 17 22:23 f1 -rw-r--r-- 1 username use300 0 Jan 17 22:23 f2 -rw-r--r-- 1 username use300 0 Jan 17 22:23 f3 [username@login01 testdir2]$ ls -al dir2 total 3 drwxr-xr-x 2 username use300 5 Jan 17 22:17 . drwxr-xr-x 4 username use300 4 Jan 17 22:12 .. -rw-r--r-- 1 username use300 0 Jan 17 22:16 file1 -rw-r--r-- 1 username use300 0 Jan 17 22:16 file2 -rw-r--r-- 1 username use300 0 Jan 17 22:17 file3 For a file, we use the remove, rm command: [username@login01 testdir2]$ [username@login01 testdir2]$ rm f3 [username@login01 testdir2]$ ll -al total 72 drwxr-xr-x 5 username use300 7 Jan 17 22:31 . drwxr-x--- 50 username use300 91 Jan 17 22:01 .. drwxr-xr-x 2 username use300 5 Jan 17 22:23 dir1 drwxr-xr-x 2 username use300 5 Jan 17 22:17 dir2 drwxr-xr-x 2 username use300 5 Jan 17 22:18 dir3 -rw-r--r-- 1 username use300 0 Jan 17 22:23 f1 -rw-r--r-- 1 username use300 0 Jan 17 22:23 f2 For an empty directory, we can use the rmdir command: [username@login01 testdir2]$ ls -al dir1 total 1 drwxr-xr-x 2 username use300 2 Jan 17 22:11 . drwxr-xr-x 5 username use300 5 Jan 17 22:11 .. [username@login01 testdir2]$ rm dir1 rm: cannot remove 'dir1': Is a directory [username@login01 testdir2]$ rmdir dir1 [username@login01 testdir2]$ ll total 71 drwxr-xr-x 4 username use300 4 Jan 17 22:12 . drwxr-x--- 50 username use300 91 Jan 17 22:01 .. drwxr-xr-x 2 username use300 2 Jan 17 22:11 dir2 drwxr-xr-x 2 username use300 2 Jan 17 22:11 dir3 If the directory has contents (files or subdirectories), you use the 'rm' command with arguments to force the removal of the directory and all of its contents: [username@login01 testdir2]$ ll dir3 total 3 drwxr-xr-x 2 username use300 5 Jan 17 22:18 . drwxr-xr-x 5 username use300 7 Jan 17 22:31 .. -rw-r--r-- 1 username use300 0 Jan 17 22:18 file31 -rw-r--r-- 1 username use300 0 Jan 17 22:18 file32 -rw-r--r-- 1 username use300 0 Jan 17 22:18 file33 [username@login01 testdir2]$ rmdir dir3 rmdir: failed to remove 'dir3': Directory not empty [username@login01 testdir2]$ rm dir3 rm: cannot remove 'dir3': Is a directory [username@login01 testdir2]$ rm -f dir3 rm: cannot remove 'dir3': Is a directory [username@login01 testdir2]$ rm -rf dir3 [username@login01 testdir2]$ ls -al total 72 drwxr-xr-x 4 username use300 6 Jan 17 23:01 . drwxr-x--- 50 username use300 91 Jan 17 22:01 .. drwxr-xr-x 2 username use300 5 Jan 17 22:23 dir1 drwxr-xr-x 2 username use300 5 Jan 17 22:17 dir2 -rw-r--r-- 1 username use300 0 Jan 17 22:23 f1 -rw-r--r-- 1 username use300 0 Jan 17 22:23 f2 Back to Top Copying directories A common task in computing is to work with examples and collaborator files. Suppose we want to copy the contents of another directory to our local directory. On Expanse, there is a large suite of applications that you can work with. In this example, we will copy the GPU application folder. Suppose you are interested in working with one of the files or directories in the /share/apps/examples/ directory. First, we will make a folder to hold expanse examples and then cd into that new directory. This is done using the mkdir command: [username@login02 ~]$ mkdir expanse-examples [username@login02 ~]$ ls -al total 166 drwxr-x--- 8 username abc123 24 Jul 17 20:20 . drwxr-xr-x 139 root root 0 Jul 17 20:17 .. -rw-r--r-- 1 username abc123 2487 Jun 23 2023 .alias -rw------- 1 username abc123 14247 Jul 17 12:11 .bash_history -rw-r--r-- 1 username abc123 18 Jun 19 2023 .bash_logout -rw-r--r-- 1 username abc123 176 Jun 19 2023 .bash_profile -rw-r--r-- 1 username abc123 159 Jul 17 18:24 .bashrc drwxr-xr-x 2 username abc123 2 Jul 17 20:20 expanse-examples [snip extra lines] [username@login02 ~]$ cd expanse-examples/ [username@login02 expanse-examples]$ pwd /home/username/expanse-examples [username@login02 expanse-examples]$ Next, we will look at the directory where the OPENMP code is stored: [username@login02 ~]$ ls -al /share/apps/examples/OPENMP/ total 740 drwxrwxr-x 2 mahidhar abc123 4096 Mar 12 08:54 . drwxrwxr-x 56 mahidhar abc123 4096 May 14 18:11 .. -rwxr-xr-x 1 mahidhar abc123 728112 Apr 15 2015 hello_openmp -rw-r--r-- 1 mahidhar abc123 984 Apr 15 2015 hello_openmp.500005.expanse-27-01.out -rw-r--r-- 1 mahidhar abc123 247 Apr 15 2015 hello_openmp.f90 -rw-r--r-- 1 mahidhar abc123 656 Apr 22 2015 hello_openmp_shared.508392.expanse-11-01.out -rw-r--r-- 1 mahidhar abc123 310 Apr 15 2015 openmp-slurm.sb -rw-r--r-- 1 mahidhar abc123 347 Apr 22 2015 openmp-slurm-shared.sb Copies of files and directories use the same command: cp . To copy a single file to the expanse-examples directory, we need to use the full path: [username@login02 expanse-examples]$ cp /share/apps/examples/OPENMP/hello_openmp.f90 hello_openmp.f90 [username@login02 expanse-examples]$ ls -al total 29 drwxr-xr-x 2 username abc123 3 Jul 17 20:24 . drwxr-x--- 8 username abc123 24 Jul 17 20:20 .. -rw-r--r-- 1 username abc123 247 Jul 17 20:24 hello_openmp.f90 For a large number of files, it is easier to copy the entire directory using the -R or -r recursive command: [username@login02 expanse-examples]$ cp -r -p /share/apps/examples/OPENMP/ . [username@login02 expanse-examples]$ ll total 48 drwxr-xr-x 3 username abc123 4 Jul 17 20:26 . drwxr-x--- 8 username abc123 24 Jul 17 20:20 .. -rw-r--r-- 1 username abc123 247 Jul 17 20:24 hello_openmp.f90 drwxr-xr-x 2 username abc123 8 Jul 17 20:26 OPENMP [username@login02 expanse-examples]$ ls -al OPENMP/ total 479 drwxrwxr-x 2 username abc123 8 Mar 12 08:54 . drwxr-xr-x 3 username abc123 4 Jul 17 20:32 .. -rwxr-xr-x 1 username abc123 728112 Apr 15 2015 hello_openmp -rw-r--r-- 1 username abc123 984 Apr 15 2015 hello_openmp.500005.expanse-27-01.out -rw-r--r-- 1 username abc123 247 Apr 15 2015 hello_openmp.f90 -rw-r--r-- 1 username abc123 656 Apr 22 2015 hello_openmp_shared.508392.expanse-11-01.out -rw-r--r-- 1 username abc123 310 Apr 15 2015 openmp-slurm.sb -rw-r--r-- 1 username abc123 347 Apr 22 2015 openmp-slurm-shared.sb There are several things to observe with this command: 1. The owner of these new files is the user who ran the commands (username). 2. The use of the -r argument is a recursive copy, which gets all files in the directory. 3. The use of the -p arguement preserves the date/timestamp, which can be helpful but not always required. 4. The use of one of the special dot characters, . in the command above: the syntax tells the operating system to copy all contents of the /share/apps/examples/OPENMP/ directory to the . directory, or the current directory, which in this case is: [username@login02 expanse-examples]$ pwd /home/username/expanse-examples [username@login02 expanse-examples]$ ls -al total 48 drwxr-xr-x 3 username abc123 4 Jul 17 20:32 . drwxr-x--- 8 username abc123 24 Jul 17 20:20 .. -rw-r--r-- 1 username abc123 247 Jul 17 20:24 hello_openmp.f90 drwxr-xr-x 2 username abc123 8 Jul 17 20:26 OPENMP [username@login02 expanse-examples]$ ls -al . total 48 drwxr-xr-x 3 username abc123 4 Jul 17 20:32 . drwxr-x--- 8 username abc123 24 Jul 17 20:20 .. -rw-r--r-- 1 username abc123 247 Jul 17 20:24 hello_openmp.f90 drwxr-xr-x 2 username abc123 8 Jul 17 20:26 OPENMP You can also copy a file or directory and give it a new name: [username@login02 expanse-examples]$ cp -r -p /share/apps/examples/OPENMP/ FOOBAR [username@login02 expanse-examples]$ ll total 49 drwxr-xr-x 4 username abc123 5 Jul 17 21:19 . drwxr-x--- 9 username abc123 26 Jul 17 21:04 .. -rw-r--r-- 1 username abc123 247 Jul 17 20:24 hello_openmp.f90 drwxrwxr-x 2 username abc123 8 Mar 12 08:54 OPENMP drwxrwxr-x 2 username abc123 8 Mar 12 08:54 FOOBAR [username@login02 expanse-examples]$ ll FOOBAR total 488 drwxrwxr-x 2 username abc123 8 Mar 12 08:54 . drwxr-xr-x 4 username abc123 5 Jul 17 21:19 .. -rwxr-xr-x 1 username abc123 728112 Apr 15 2015 hello_openmp -rw-r--r-- 1 username abc123 984 Apr 15 2015 hello_openmp.500005.expanse-27-01.out -rw-r--r-- 1 username abc123 247 Apr 15 2015 hello_openmp.f90 -rw-r--r-- 1 username abc123 656 Apr 22 2015 hello_openmp_shared.508392.expanse-11-01.out -rw-r--r-- 1 username abc123 310 Apr 15 2015 openmp-slurm.sb -rw-r--r-- 1 username abc123 347 Apr 22 2015 openmp-slurm-shared.sb You can rename a directory using the mv command: [username@login02 expanse-examples]$ mv FOOBAR/ OPENMP_DUP [username@login02 expanse-examples]$ /bin/ls -l total 48 -rw-r--r-- 1 username abc123 247 Jul 17 20:24 hello_openmp.f90 drwxrwxr-x 2 username abc123 8 Mar 12 08:54 OPENMP drwxrwxr-x 2 username abc123 8 Mar 12 08:54 OPENMP_DUP [username@login02 expanse-examples]$ To move to the directory, use the cd (change directory) [username@login02 expanse-examples]$ pwd /home/username/expanse-examples [username@login02 expanse-examples]$ cd OPENMP/ [username@login02 OPENMP]$ pwd /home/username/expanse-examples/OPENMP [username@login02 OPENMP]$ ls -al total 479 drwxr-xr-x 2 username abc123 8 Jul 17 20:26 . drwxr-xr-x 3 username abc123 4 Jul 17 20:32 .. -rwxr-xr-x 1 username abc123 728112 Jul 17 20:26 hello_openmp -rw-r--r-- 1 username abc123 984 Jul 17 20:26 hello_openmp.500005.expanse-27-01.out -rw-r--r-- 1 username abc123 247 Jul 17 20:26 hello_openmp.f90 -rw-r--r-- 1 username abc123 656 Jul 17 20:26 hello_openmp_shared.508392.expanse-11-01.out -rw-r--r-- 1 username abc123 310 Jul 17 20:26 openmp-slurm.sb -rw-r--r-- 1 username abc123 347 Jul 17 20:26 openmp-slurm-shared.sb You can sort the order of the file listings by date (or size or other fields -- see the man pages). The default file listing in alphabetic, to see the files in chronological order (or reverse): [username@login02 expanse-examples]$ ls -alt OPENMP/ total 479 drwxr-xr-x 4 username abc123 5 Jul 17 20:43 .. drwxrwxr-x 2 username abc123 8 Mar 12 08:54 . -rw-r--r-- 1 username abc123 656 Apr 22 2015 hello_openmp_shared.508392.expanse-11-01.out -rw-r--r-- 1 username abc123 347 Apr 22 2015 openmp-slurm-shared.sb -rw-r--r-- 1 username abc123 984 Apr 15 2015 hello_openmp.500005.expanse-27-01.out -rwxr-xr-x 1 username abc123 728112 Apr 15 2015 hello_openmp -rw-r--r-- 1 username abc123 247 Apr 15 2015 hello_openmp.f90 -rw-r--r-- 1 username abc123 310 Apr 15 2015 openmp-slurm.sb [username@login02 expanse-examples]$ ls -altr OPENMP/ total 479 -rw-r--r-- 1 username abc123 310 Apr 15 2015 openmp-slurm.sb -rw-r--r-- 1 username abc123 247 Apr 15 2015 hello_openmp.f90 -rwxr-xr-x 1 username abc123 728112 Apr 15 2015 hello_openmp -rw-r--r-- 1 username abc123 984 Apr 15 2015 hello_openmp.500005.expanse-27-01.out -rw-r--r-- 1 username abc123 347 Apr 22 2015 openmp-slurm-shared.sb -rw-r--r-- 1 username abc123 656 Apr 22 2015 hello_openmp_shared.508392.expanse-11-01.out drwxrwxr-x 2 username abc123 8 Mar 12 08:54 . drwxr-xr-x 4 username abc123 5 Jul 17 20:43 .. Back to Top Manipulating Files This section will show you more ways to manipulate files: copying, listing, deleting and renaming, and examining contents In the section above, we created files using the touch commmand, which will create a file with no contents. First we'll return to the first testdir using the full directory path : [username@login01 testdir2]$ cd /home/username/testdir [username@login02 testdir]$ touch myfile1.txt [username@login02 testdir]$ touch myfile2.txt [username@login02 testdir]$ ls -l total 21 drwxr-xr-x 2 username abc123 4 Jul 17 20:53 . drwxr-x--- 9 username abc123 25 Jul 17 20:49 .. -rw-r--r-- 1 username abc123 0 Jul 17 20:53 myfile1.txt -rw-r--r-- 1 username abc123 0 Jul 17 20:53 myfile2.txt To copy a file from another directory to the current directory, use the full path. In this example, we will copy the filelisting.txt file from the directory above, using the .. (\\\"dot-dot\\\") variable for the directory location: [username@login02 testdir]$ cp ../filelisting.txt . [username@login02 testdir]$ ls -l total 11 -rw-r--r-- 1 username abc123 322 Jul 17 21:09 filelisting.txt -rw-r--r-- 1 username abc123 0 Jul 17 20:53 myfile1.txt -rw-r--r-- 1 username abc123 0 Jul 17 20:53 myfile2.txt To rename a file, use the mv (move) command: [username@login02 testdir]$ mv myfile2.txt newfile.txt [username@login02 testdir]$ ls -l total 11 -rw-r--r-- 1 username abc123 1543 Jul 17 21:09 filelisting.txt -rw-r--r-- 1 username abc123 0 Jul 17 20:53 myfile1.txt -rw-r--r-- 1 username abc123 0 Jul 17 20:53 newfile.txt To delete a file, use the rm (remove command): [username@login02 testdir]$ rm myfile1.txt [username@login02 testdir]$ ls -l total 10 -rw-r--r-- 1 username abc123 1543 Jul 17 21:09 filelisting.txt -rw-r--r-- 1 username abc123 0 Jul 17 20:53 newfile.txt -rw-r--r-- 1 username use300 1410 Jan 18 00:00 sdsc.txt You can examine the contents of a file by using several Linux commands. First, we'll create a small file for testing: [username@login01 testdir]$ ls -al > dirlist.txt [username@login01 testdir]$ ls -al total 88 drwxr-xr-x 2 username use300 5 Jan 17 23:49 . drwxr-x--- 51 username use300 93 Jan 17 23:46 .. -rw-r--r-- 1 username use300 284 Jan 17 23:49 dirlist.txt -rw-r--r-- 1 username use300 322 Jan 17 23:42 filelisting.txt -rw-r--r-- 1 username use300 0 Jan 17 23:42 newfile.txt -rw-r--r-- 1 username use300 1410 Jan 18 00:00 sdsc.txt To print out the contents of the entire file, use the cat command (cat - concatenate files and print on the standard output): [username@login01 testdir]$ cat sdsc.txt The San Diego Supercomputer Center (SDSC) was established as one of the nation\u2019s first supercomputer centers under a cooperative agreement by the National Science Foundation (NSF) in collaboration with UC San Diego and General Atomics (GA) Technologies. SDSC first opened its doors on November 14, 1985. For more than 35 years, it has grown and stewarded its national reputation as a pioneer and leader in high-performance and data-intensive computing and cyberinfrastructure. Located on the campus of UC San Diego, SDSC provides resources, services and expertise to the national research community including industry and academia. Cyberinfrastructure refers to an accessible, integrated network of computer-based resources and expertise, focused on accelerating scientific inquiry and discovery. With Voyager and Expanse, SDSC\u2019s latest supercomputing resources, the center supports hundreds of multidisciplinary programs spanning a wide range of science themes\u2014from earth sciences and biology to astrophysics, bioinformatics and health IT. SDSC participates in Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) and was a partner in eXtreme Science and Engineering Discovery Environment (XSEDE), two of the most advanced collections of integrated digital resources and services in the world. For general inquiries and comments: info@sdsc.edu SDSC website: www.sdsc.edu The contents of a file may be too large to see at one time, to see how large it is we can run the wc , word count command: [username@login01 testdir]$ wc ../README.md 644 4531 30519 ../README.md The output says that README file has 644 lines, 4531 words, and 30519 characters. We might want to look at the beginning (head) or end (tail): [username@login01 testdir]$ head -n 1 sdsc.txt The San Diego Supercomputer Center (SDSC) was established as one of the nation\u2019s first supercomputer centers under a cooperative agreement by the National Science Foundation (NSF) in collaboration with UC San Diego and General Atomics (GA) Technologies. SDSC first opened its doors on November 14, 1985. [username@login01 testdir]$ [username@login01 testdir]$ tail -n 2 sdsc.txt For general inquiries and comments: info@sdsc.edu SDSC website: www.sdsc.edu You can reorder the contents of a file using the sort command: [mthomas@login01 testdir]$ sort sdsc.txt Cyberinfrastructure refers to an accessible, integrated network of computer-based resources and expertise, focused on accelerating scientific inquiry and discovery. With Voyager and Expanse, SDSC\u2019s latest supercomputing resources, the center supports hundreds of multidisciplinary programs spanning a wide range of science themes\u2014from earth sciences and biology to astrophysics, bioinformatics and health IT. For general inquiries and comments: info@sdsc.edu For more than 35 years, it has grown and stewarded its national reputation as a pioneer and leader in high-performance and data-intensive computing and cyberinfrastructure. Located on the campus of UC San Diego, SDSC provides resources, services and expertise to the national research community including industry and academia. SDSC participates in Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) and was a partner in eXtreme Science and Engineering Discovery Environment (XSEDE), two of the most advanced collections of integrated digital resources and services in the world. SDSC website: www.sdsc.edu The San Diego Supercomputer Center (SDSC) was established as one of the nation\u2019s first supercomputer centers under a cooperative agreement by the National Science Foundation (NSF) in collaboration with UC San Diego and General Atomics (GA) Technologies. SDSC first opened its doors on November 14, 1985. The sort command is useful for sorting through a long file and reording data. To search a file for a string you can use the grep command: [mthomas@login01 testdir]$ grep -ni SDSC sdsc.txt 1:The San Diego Supercomputer Center (SDSC) was established as one of the nation\u2019s first supercomputer centers under a cooperative agreement by the National Science Foundation (NSF) in collaboration with UC San Diego and General Atomics (GA) Technologies. SDSC first opened its doors on November 14, 1985. 3:For more than 35 years, it has grown and stewarded its national reputation as a pioneer and leader in high-performance and data-intensive computing and cyberinfrastructure. Located on the campus of UC San Diego, SDSC provides resources, services and expertise to the national research community including industry and academia. 5:Cyberinfrastructure refers to an accessible, integrated network of computer-based resources and expertise, focused on accelerating scientific inquiry and discovery. With Voyager and Expanse, SDSC\u2019s latest supercomputing resources, the center supports hundreds of multidisciplinary programs spanning a wide range of science themes\u2014from earth sciences and biology to astrophysics, bioinformatics and health IT. 7:SDSC participates in Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) and was a partner in eXtreme Science and Engineering Discovery Environment (XSEDE), two of the most advanced collections of integrated digital resources and services in the world. 9:For general inquiries and comments: info@sdsc.edu 10:SDSC website: www.sdsc.edu In the command above, we used the grep -ni argument to get the line number, and to ignore case. Back to Top Permissions In the section we will look breifly at how to set file permissions. Before you can change the file permissions, you need to own it or have permission as a 2023 member. For a more detailed tutorial, see http://www.nersc.gov/users/storage-and-file-systems/unix-file-permissions/. Permissions are written in the first column, with fields that specify whether or not the file is a directory ( d ), what the read/write/execution permissions ( rwx ) for the files are for users and groups. Using the example below: [username@login02 OPENMP]$ ls -l hello_openmp total 479 drwxr-xr-x 2 username abc123 2 Jul 17 21:53 direxample -rwxr-xr-x 1 username abc123 728112 Apr 15 2015 hello_openmp -rw-r--r-- 1 username abc123 247 Apr 15 2015 hello_openmp.f90 The order of the markers are grouped into 4 fields: - rwx r-x r-x Field 1 == directory, a d or - means directory or not a directory Field 2 == owner permissions 'rwx' means the owner can read, write, and exectute Field 3 == 2023 permissions 'rwx' means the owner can read and exectute, but not modify Field 4 == other/world permissions 'r-x' means the others can read and exectute, but not modiry To change the file access permissions, use the chmod command. In the example below, only user username has permission to edit ( rw- ) the files, members of the 2023 abc123 and others have read only permission ( -- ). There are several ways to modify permissions, we will use the binary representation where the rwx status represents a binary number 2^n, where n is the position of the permission starting from the right. For example: r-- = 2^2 + 0 + 0 = 4 + 0 + 0 = 4 rw- = 2^2 + 2^1 + 0 = 4 + 2 + 0 = 6 r-x = 2^2 + 0 + 2^0 = 4 + 0 + 1 = 5 rwx = 2^2 + 2^1 + 2^0 = 4 + 2 + 1 = 7 In the example below, we will set read and write permissions to the owner and the group, and limit the other/world 2023 to read only: [username@login02 OPENMP]$ ls -l total 479 drwxr-xr-x 2 username abc123 2 Jul 17 21:53 direxample -rwxr-xr-x 1 username abc123 728112 Apr 15 2015 hello_openmp -rw-r--r-- 1 username abc123 984 Apr 15 2015 hello_openmp.500005.expanse-27-01.out -rw-r--r-- 1 username abc123 247 Apr 15 2015 hello_openmp.f90 -rw-r--r-- 1 username abc123 656 Apr 22 2015 hello_openmp_shared.508392.expanse-11-01.out -rw-r--r-- 1 username abc123 310 Apr 15 2015 openmp-slurm.sb -rw-r--r-- 1 username abc123 347 Apr 22 2015 openmp-slurm-shared.sb [username@login02 OPENMP]$ chmod 660 * [username@login02 OPENMP]$ ls -l total 460 drwxr-xr-x 2 username abc123 2 Jul 17 21:53 direxample -rw-rw-r-- 1 username abc123 728112 Apr 15 2015 hello_openmp -rw-rw---- 1 username abc123 984 Apr 15 2015 hello_openmp.500005.expanse-27-01.out -rw-rw---- 1 username abc123 247 Apr 15 2015 hello_openmp.f90 -rw-rw---- 1 username abc123 656 Apr 22 2015 hello_openmp_shared.508392.expanse-11-01.out -rw-rw---- 1 username abc123 310 Apr 15 2015 openmp-slurm.sb -rw-rw---- 1 username abc123 347 Apr 22 2015 openmp-slurm-shared.sb In the example above, we use the star wildcard, \\\" * \\\" to represent all the files in the directory (See the section on wildcards below). We can use the wildcard to change the group of some of the files. For example, to change the 2023 of only the *.out files: [username@login02 OPENMP]$ groups abc123 pet heart scicom-docs grdclus webwrt ... [username@login02 OPENMP]$ chgrp heart *.out [username@login02 OPENMP]$ ls -l total 460 drwxr-xr-x 2 username abc123 2 Jul 17 21:53 direxample -rw-rw-r-- 1 username abc123 728112 Apr 15 2015 hello_openmp -rw-rw---- 1 username heart 984 Apr 15 2015 hello_openmp.500005.expanse-27-01.out -rw-rw---- 1 username abc123 247 Apr 15 2015 hello_openmp.f90 -rw-rw---- 1 username heart 656 Apr 22 2015 hello_openmp_shared.508392.expanse-11-01.out -rw-rw---- 1 username abc123 310 Apr 15 2015 openmp-slurm.sb -rw-rw---- 1 username abc123 347 Apr 22 2015 openmp-slurm-shared.sb Back to Top Wildcards \\\"A wildcard is a character that can be used as a substitute for any of a class of characters in a search, thereby greatly increasing the flexibility and efficiency of searches.\" The wildcards are very powerful, and there is not room in this document for all of them, so we recommend that you check out this site: http://www.linfo.org/wildcard.html for more information. In the example below, we use the star wildcard to list all files ending in .out [username@login02 OPENMP]$ ls -al *.out -rw-rw---- 1 username heart 984 Apr 15 2015 hello_openmp.500005.expanse-27-01.out -rw-rw---- 1 username heart 656 Apr 22 2015 hello_openmp_shared.508392.expanse-11-01.out Back to Top Other Utilities to Learj: grep, sort, tar, gzip ,and pigz","title":"Using SDSC HPC Resources: Basic Linux Skills on Expanse"},{"location":"basic_skills/basic_linux_skills_expanse/#using-sdsc-hpc-resources-basic-linux-skills-on-expanse","text":"When you log on to Expanse , your computer operating system will be a Linux or Unix shell. \\\" A Unix shell is a command-line interpreter or shell that provides a traditional Unix-like command line user interface. This environment is very different from the easy to use GUI interfaces we have all become used in the Windows and MacOS systems \\\" (https://en.wikipedia.org/wiki/Unix_shell). Basic Linux skills are necessary to complete the hands-on exercises. If it\u2019s been a while since you\u2019ve worked in a Linux environment, be sure to reacquaint yourself with the following topic (many of which are demonstrated below)s: copying, listing, deleting and renaming files; using wildcards; navigating directories; changing file permissions; setting environment variables; using common utilities (grep, cat, less, head, sort, tar, gzip). This tutorial is focussed on using Linux on the SDSC Expanse supercomputer. There a a lot of other tutorial on the Web: * A nice tutorial can be found here http://www.ee.surrey.ac.uk/Teaching/Unix/. You should also be comfortable with one of the standard Linux editors, such as vim, emacs, or nano. * For a fun tutorial on Linux, see the UCSD/SDSC Supercomputing Club's tutorial, based on a Google community project, here: https://supercomputing-club.sdsc.edu/posts/advent-of-scc-2023/advent-of-scc-epilogue/ Notes: * For the examples below, we are using the bash shell, which is the default shell for new accounts on Expanse. For the purposes of following SDSC tutorials and exercises, please do not change the shell. * For any Linux command, you can find out what it does by asking for help. The syntax is usually * command --help * man command --> invokes a user guide or manual * search the web using google or some other search engine * In Linux/Unix, everything is a file: a text file, a directory, even the output for a command * Linux/Unix is case sensitive. Examples:: * Basic Environment * Directories and Navigation * Copying directories * Files * Permissions * Wildcards * Common Utilities Note: if you have difficulties completing this task, please contact staff at consult@sdsc.edu .","title":"Using SDSC HPC Resources: Basic Linux Skills on Expanse"},{"location":"basic_skills/basic_linux_skills_expanse/#basic-environment","text":"Using Unix commands, we can learn a lot about the machine we are logged onto. Some of the commands are simple: [username@login02 ~]$ date Tue Jan 16 20:20:23 PST 2024 [username@login02 ~]$ hostname login02 [username@login02 ~]$ whoami username Note: To learn about most unix commands, try accessing the man pages: [username@login02 ~]$ man date NAME date - print or set the system date and time SYNOPSIS date [OPTION]... [+FORMAT] date [-u|--utc|--universal] [MMDDhhmm[[CC]YY][.ss]] DESCRIPTION Display the current time in the given FORMAT, or set the system date. ..... more info ..... The unix command env will print out the environment settings for your login session. The list below is an edited summary of all the information Warning: the output can be very long (over 90 lines) [username@login02 ~]$ env MODULEPATH=/opt/modulefiles/mpi/.intel:/opt/modulefiles/applications/.intel:/opt/modulefiles/mpi:/opt/modulefiles/compilers:/opt/modulefiles/applications:/usr/share/Modules/modulefiles:/etc/modulefiles LOADEDMODULES=intel/2013_sp1.2.144:mvapich2_ib/2.1:gnutools/2.69 HOME=/home/username SDSCHOME=/opt/sdsc LOGNAME=username SSH_CONNECTION=xxx.xxx.xx.xx 53640 198.202.113.253 22 DISPLAY=localhost:48.0 It is often useful to print out (or use) environment variables. To print them out, use the echo command, and $ sign (which extracts the value of the shell variable): [username@login02 ~]$ echo $SHELL /bin/bash [username@login02 ~]$ echo $HOME /home/username Another important environment variable is the home directory variable, the \\\"tilde\\\" character: ~ [username@login02 ~]$ echo ~ /home/username [username@login02 ~]$ You can create your own environment variables: [username@login02 ~]$ MY_NAME=\"Super User\" [username@login02 ~]$ echo $MY_NAME Super User Unix has the concept of users and groups. Groups are used to control access to resources (files, applications, etc.) and help establish a secure envionment Users can be in more than one group. To see which groups you are a member of, use the group command: [username@login02 OPENMP]$ groups abc123 pet heart scicom-docs grdclus webwrt scwpf ... Back to Top","title":"Basic Environment"},{"location":"basic_skills/basic_linux_skills_expanse/#directories-and-navigation","text":"In unix, everything is a file, which can be confusing at first. The locations for where files are stored are called directories (which is equivalent to folders), and are also viewed as files by the operating system. To find out where you are in the system, use the pwd command (print working directory), which prints the full path to the directory you are currently in: [username@login02 ~]$ pwd /home/username To see what are the contents of the current directory are, use the file listing command ls [username@login02 ~]$ ls filelisting.txt intel loadgccomgnuenv.sh loadgnuenv.sh loadintelenv.sh tools In every Unix directory, there are \\\"hidden\\\" files (just like on Macs and Windows machines), to see them, run the ls -a command: [username@login02 ~]$ ls -a . .bash_history .bashrc .gitconfig loadgccomgnuenv.sh .ncviewrc .ssh .vimrc .. .bash_logout .config filelisting.txt intel loadgnuenv.sh .petscconfig tools .Xauthority .alias .bash_profile .kshrc loadintelenv.sh .slurm .viminfo In Unix, sometimes it is hard tell if a file is a directory. To see file details (including timestamp and size), run the ls -l command: [username@login02 ~]$ ls -l -rw-r--r-- 1 username abc123 322 Jul 17 21:04 filelisting.txt drwxr-xr-x 3 username abc123 3 Jun 22 2023 intel -rwx------ 1 username abc123 101 Jun 27 2023 loadgccomgnuenv.sh -rwx------ 1 username abc123 77 Oct 16 2023 loadgnuenv.sh -rwxr-xr-x 1 username abc123 125 Oct 16 2023 loadintelenv.sh drwxr-xr-x 2 username abc123 4 Jun 30 2023 tools You can combine the two commands above and use it to see the full directory and file information: [username@login02 ~]$ ls -al total 166 drwx------ 7 username abc123 23 Jul 17 19:33 . drwxr-xr-x 143 root root 0 Jul 17 20:01 .. -rw-r--r-- 1 username abc123 2487 Jun 23 2023 .alias -rw------- 1 username abc123 14247 Jul 17 12:11 .bash_history -rw-r--r-- 1 username abc123 18 Jun 19 2023 .bash_logout -rw-r--r-- 1 username abc123 176 Jun 19 2023 .bash_profile -rw-r--r-- 1 username abc123 159 Jul 17 18:24 .bashrc drwx------ 3 username abc123 3 Oct 23 2023 .config -rw-r--r-- 1 username abc123 322 Jul 17 21:04 filelisting.txt -rw-r--r-- 1 username abc123 1641 Jun 22 2023 .gccomrc -rw-r--r-- 1 username abc123 245 Jun 28 2023 .gitconfig drwxr-xr-x 3 username abc123 3 Jun 22 2023 intel -rw-r--r-- 1 username abc123 171 Jun 19 2023 .kshrc -rwx------ 1 username abc123 101 Jun 27 2023 loadgccomgnuenv.sh -rwx------ 1 username abc123 77 Oct 16 2023 loadgnuenv.sh -rwxr-xr-x 1 username abc123 125 Oct 16 2023 loadintelenv.sh [snip extra lines] There are several things to notice in the above listing: the first column of data is information about the file \"permissions\\\", which controls who can see/read/modify what files ( r =read, w =write, x =execute, - =no permission); the next 2 columns are the username and groupID; the 3rd and 4th columns are the size and date. This is discussed in more detail in the Permissions section below. Also, note that two files have dots for their names: in unix the \"dot\" is a component of a filename. When working with filenames, a leading dot is the prefix of a \"hidden\" file, a file that an ls will not normally show. But also, the single dot, . represents the current working directory, and the double dots, .. represent the directory above. You use these as arguments to unix commands dealing with directories. There are simple Linux commands to create and remove directories, and to populate the directories. To create a directory, use the mkdir , make directory command (more about directories in the sections below): [username@login02 ~]$ mkdir testdir [username@login02 ~]$ ls -l total 12 drwxr-xr-x 3 username abc123 3 Jun 22 2023 intel -rwx------ 1 username abc123 101 Jun 27 2023 loadgccomgnuenv.sh -rwx------ 1 username abc123 77 Oct 16 2023 loadgnuenv.sh -rwxr-xr-x 1 username abc123 125 Oct 16 2023 loadintelenv.sh drwxr-xr-x 2 username abc123 2 Jul 17 20:49 testdir drwxr-xr-x 2 username abc123 4 Jun 30 2023 tools To move into that directory, use the cd , change directory command: [username@login02 ~]$ cd testdir/ [username@login02 testdir]$ ls -al total 20 drwxr-xr-x 2 username abc123 2 Jul 17 20:49 . drwxr-x--- 9 username abc123 25 Jul 17 20:49 .. [username@login02 testdir]$ From this directory, you can use the .. command to see the contents of the directory above: [username@login02 testdir]$ ls -l .. [username@login02 testdir]$ /bin/ls -l .. total 22 drwxr-xr-x 4 username abc123 5 Jul 17 20:43 expanse-examples -rw-r--r-- 1 username abc123 322 Jul 17 21:04 filelisting.txt drwxr-xr-x 3 username abc123 3 Jun 22 2023 intel -rwx------ 1 username abc123 101 Jun 27 2023 loadgccomgnuenv.sh -rwx------ 1 username abc123 77 Oct 16 2023 loadgnuenv.sh -rwxr-xr-x 1 username abc123 125 Oct 16 2023 loadintelenv.sh drwxr-xr-x 2 username abc123 4 Jul 17 20:53 testdir drwxr-xr-x 2 username abc123 4 Jun 30 2023 tools To remove files and directories there are different mechanisms, depending on whether or not the directory is empty or contains files. For this example, we create a new directory, testdir2 , we'll create three directories using the mkdir command, populate some testfiles using the touch command, and then try to delete the directories using either the rmdir or rm commands. [username@login01 testdir]$cd .. [username@login01 testdir]$ mkdir testdir2 [username@login01 testdir]$ cd testdir2 [username@login01 testdir2]$ mkdir dir1 [username@login01 testdir2]$ mkdir dir2 [username@login01 testdir2]$ mkdir dir3 [username@login01 testdir2]$ ll total 71 drwxr-xr-x 5 username use300 5 Jan 17 22:11 . drwxr-x--- 50 username use300 91 Jan 17 22:01 .. drwxr-xr-x 2 username use300 2 Jan 17 22:11 dir1 drwxr-xr-x 2 username use300 2 Jan 17 22:11 dir2 drwxr-xr-x 2 username use300 2 Jan 17 22:11 dir3 Next, create some testfiles using the touch command: [username@login01 testdir2]$ touch f1 [username@login01 testdir2]$ touch f2 [username@login01 testdir2]$ touch f3 [username@login01 testdir2]$ touch dir2/file1 [username@login01 testdir2]$ touch dir2/file2 [username@login01 testdir2]$ touch dir2/file3 [username@login01 testdir2]$ ls -al total 90 drwxr-xr-x 5 username use300 8 Jan 17 22:23 . drwxr-x--- 50 username use300 91 Jan 17 22:01 .. drwxr-xr-x 2 username use300 5 Jan 17 22:23 dir1 drwxr-xr-x 2 username use300 5 Jan 17 22:17 dir2 drwxr-xr-x 2 username use300 5 Jan 17 22:18 dir3 -rw-r--r-- 1 username use300 0 Jan 17 22:23 f1 -rw-r--r-- 1 username use300 0 Jan 17 22:23 f2 -rw-r--r-- 1 username use300 0 Jan 17 22:23 f3 [username@login01 testdir2]$ ls -al dir2 total 3 drwxr-xr-x 2 username use300 5 Jan 17 22:17 . drwxr-xr-x 4 username use300 4 Jan 17 22:12 .. -rw-r--r-- 1 username use300 0 Jan 17 22:16 file1 -rw-r--r-- 1 username use300 0 Jan 17 22:16 file2 -rw-r--r-- 1 username use300 0 Jan 17 22:17 file3 For a file, we use the remove, rm command: [username@login01 testdir2]$ [username@login01 testdir2]$ rm f3 [username@login01 testdir2]$ ll -al total 72 drwxr-xr-x 5 username use300 7 Jan 17 22:31 . drwxr-x--- 50 username use300 91 Jan 17 22:01 .. drwxr-xr-x 2 username use300 5 Jan 17 22:23 dir1 drwxr-xr-x 2 username use300 5 Jan 17 22:17 dir2 drwxr-xr-x 2 username use300 5 Jan 17 22:18 dir3 -rw-r--r-- 1 username use300 0 Jan 17 22:23 f1 -rw-r--r-- 1 username use300 0 Jan 17 22:23 f2 For an empty directory, we can use the rmdir command: [username@login01 testdir2]$ ls -al dir1 total 1 drwxr-xr-x 2 username use300 2 Jan 17 22:11 . drwxr-xr-x 5 username use300 5 Jan 17 22:11 .. [username@login01 testdir2]$ rm dir1 rm: cannot remove 'dir1': Is a directory [username@login01 testdir2]$ rmdir dir1 [username@login01 testdir2]$ ll total 71 drwxr-xr-x 4 username use300 4 Jan 17 22:12 . drwxr-x--- 50 username use300 91 Jan 17 22:01 .. drwxr-xr-x 2 username use300 2 Jan 17 22:11 dir2 drwxr-xr-x 2 username use300 2 Jan 17 22:11 dir3 If the directory has contents (files or subdirectories), you use the 'rm' command with arguments to force the removal of the directory and all of its contents: [username@login01 testdir2]$ ll dir3 total 3 drwxr-xr-x 2 username use300 5 Jan 17 22:18 . drwxr-xr-x 5 username use300 7 Jan 17 22:31 .. -rw-r--r-- 1 username use300 0 Jan 17 22:18 file31 -rw-r--r-- 1 username use300 0 Jan 17 22:18 file32 -rw-r--r-- 1 username use300 0 Jan 17 22:18 file33 [username@login01 testdir2]$ rmdir dir3 rmdir: failed to remove 'dir3': Directory not empty [username@login01 testdir2]$ rm dir3 rm: cannot remove 'dir3': Is a directory [username@login01 testdir2]$ rm -f dir3 rm: cannot remove 'dir3': Is a directory [username@login01 testdir2]$ rm -rf dir3 [username@login01 testdir2]$ ls -al total 72 drwxr-xr-x 4 username use300 6 Jan 17 23:01 . drwxr-x--- 50 username use300 91 Jan 17 22:01 .. drwxr-xr-x 2 username use300 5 Jan 17 22:23 dir1 drwxr-xr-x 2 username use300 5 Jan 17 22:17 dir2 -rw-r--r-- 1 username use300 0 Jan 17 22:23 f1 -rw-r--r-- 1 username use300 0 Jan 17 22:23 f2 Back to Top","title":"Directories and Navigation"},{"location":"basic_skills/basic_linux_skills_expanse/#copying-directories","text":"A common task in computing is to work with examples and collaborator files. Suppose we want to copy the contents of another directory to our local directory. On Expanse, there is a large suite of applications that you can work with. In this example, we will copy the GPU application folder. Suppose you are interested in working with one of the files or directories in the /share/apps/examples/ directory. First, we will make a folder to hold expanse examples and then cd into that new directory. This is done using the mkdir command: [username@login02 ~]$ mkdir expanse-examples [username@login02 ~]$ ls -al total 166 drwxr-x--- 8 username abc123 24 Jul 17 20:20 . drwxr-xr-x 139 root root 0 Jul 17 20:17 .. -rw-r--r-- 1 username abc123 2487 Jun 23 2023 .alias -rw------- 1 username abc123 14247 Jul 17 12:11 .bash_history -rw-r--r-- 1 username abc123 18 Jun 19 2023 .bash_logout -rw-r--r-- 1 username abc123 176 Jun 19 2023 .bash_profile -rw-r--r-- 1 username abc123 159 Jul 17 18:24 .bashrc drwxr-xr-x 2 username abc123 2 Jul 17 20:20 expanse-examples [snip extra lines] [username@login02 ~]$ cd expanse-examples/ [username@login02 expanse-examples]$ pwd /home/username/expanse-examples [username@login02 expanse-examples]$ Next, we will look at the directory where the OPENMP code is stored: [username@login02 ~]$ ls -al /share/apps/examples/OPENMP/ total 740 drwxrwxr-x 2 mahidhar abc123 4096 Mar 12 08:54 . drwxrwxr-x 56 mahidhar abc123 4096 May 14 18:11 .. -rwxr-xr-x 1 mahidhar abc123 728112 Apr 15 2015 hello_openmp -rw-r--r-- 1 mahidhar abc123 984 Apr 15 2015 hello_openmp.500005.expanse-27-01.out -rw-r--r-- 1 mahidhar abc123 247 Apr 15 2015 hello_openmp.f90 -rw-r--r-- 1 mahidhar abc123 656 Apr 22 2015 hello_openmp_shared.508392.expanse-11-01.out -rw-r--r-- 1 mahidhar abc123 310 Apr 15 2015 openmp-slurm.sb -rw-r--r-- 1 mahidhar abc123 347 Apr 22 2015 openmp-slurm-shared.sb Copies of files and directories use the same command: cp . To copy a single file to the expanse-examples directory, we need to use the full path: [username@login02 expanse-examples]$ cp /share/apps/examples/OPENMP/hello_openmp.f90 hello_openmp.f90 [username@login02 expanse-examples]$ ls -al total 29 drwxr-xr-x 2 username abc123 3 Jul 17 20:24 . drwxr-x--- 8 username abc123 24 Jul 17 20:20 .. -rw-r--r-- 1 username abc123 247 Jul 17 20:24 hello_openmp.f90 For a large number of files, it is easier to copy the entire directory using the -R or -r recursive command: [username@login02 expanse-examples]$ cp -r -p /share/apps/examples/OPENMP/ . [username@login02 expanse-examples]$ ll total 48 drwxr-xr-x 3 username abc123 4 Jul 17 20:26 . drwxr-x--- 8 username abc123 24 Jul 17 20:20 .. -rw-r--r-- 1 username abc123 247 Jul 17 20:24 hello_openmp.f90 drwxr-xr-x 2 username abc123 8 Jul 17 20:26 OPENMP [username@login02 expanse-examples]$ ls -al OPENMP/ total 479 drwxrwxr-x 2 username abc123 8 Mar 12 08:54 . drwxr-xr-x 3 username abc123 4 Jul 17 20:32 .. -rwxr-xr-x 1 username abc123 728112 Apr 15 2015 hello_openmp -rw-r--r-- 1 username abc123 984 Apr 15 2015 hello_openmp.500005.expanse-27-01.out -rw-r--r-- 1 username abc123 247 Apr 15 2015 hello_openmp.f90 -rw-r--r-- 1 username abc123 656 Apr 22 2015 hello_openmp_shared.508392.expanse-11-01.out -rw-r--r-- 1 username abc123 310 Apr 15 2015 openmp-slurm.sb -rw-r--r-- 1 username abc123 347 Apr 22 2015 openmp-slurm-shared.sb There are several things to observe with this command: 1. The owner of these new files is the user who ran the commands (username). 2. The use of the -r argument is a recursive copy, which gets all files in the directory. 3. The use of the -p arguement preserves the date/timestamp, which can be helpful but not always required. 4. The use of one of the special dot characters, . in the command above: the syntax tells the operating system to copy all contents of the /share/apps/examples/OPENMP/ directory to the . directory, or the current directory, which in this case is: [username@login02 expanse-examples]$ pwd /home/username/expanse-examples [username@login02 expanse-examples]$ ls -al total 48 drwxr-xr-x 3 username abc123 4 Jul 17 20:32 . drwxr-x--- 8 username abc123 24 Jul 17 20:20 .. -rw-r--r-- 1 username abc123 247 Jul 17 20:24 hello_openmp.f90 drwxr-xr-x 2 username abc123 8 Jul 17 20:26 OPENMP [username@login02 expanse-examples]$ ls -al . total 48 drwxr-xr-x 3 username abc123 4 Jul 17 20:32 . drwxr-x--- 8 username abc123 24 Jul 17 20:20 .. -rw-r--r-- 1 username abc123 247 Jul 17 20:24 hello_openmp.f90 drwxr-xr-x 2 username abc123 8 Jul 17 20:26 OPENMP You can also copy a file or directory and give it a new name: [username@login02 expanse-examples]$ cp -r -p /share/apps/examples/OPENMP/ FOOBAR [username@login02 expanse-examples]$ ll total 49 drwxr-xr-x 4 username abc123 5 Jul 17 21:19 . drwxr-x--- 9 username abc123 26 Jul 17 21:04 .. -rw-r--r-- 1 username abc123 247 Jul 17 20:24 hello_openmp.f90 drwxrwxr-x 2 username abc123 8 Mar 12 08:54 OPENMP drwxrwxr-x 2 username abc123 8 Mar 12 08:54 FOOBAR [username@login02 expanse-examples]$ ll FOOBAR total 488 drwxrwxr-x 2 username abc123 8 Mar 12 08:54 . drwxr-xr-x 4 username abc123 5 Jul 17 21:19 .. -rwxr-xr-x 1 username abc123 728112 Apr 15 2015 hello_openmp -rw-r--r-- 1 username abc123 984 Apr 15 2015 hello_openmp.500005.expanse-27-01.out -rw-r--r-- 1 username abc123 247 Apr 15 2015 hello_openmp.f90 -rw-r--r-- 1 username abc123 656 Apr 22 2015 hello_openmp_shared.508392.expanse-11-01.out -rw-r--r-- 1 username abc123 310 Apr 15 2015 openmp-slurm.sb -rw-r--r-- 1 username abc123 347 Apr 22 2015 openmp-slurm-shared.sb You can rename a directory using the mv command: [username@login02 expanse-examples]$ mv FOOBAR/ OPENMP_DUP [username@login02 expanse-examples]$ /bin/ls -l total 48 -rw-r--r-- 1 username abc123 247 Jul 17 20:24 hello_openmp.f90 drwxrwxr-x 2 username abc123 8 Mar 12 08:54 OPENMP drwxrwxr-x 2 username abc123 8 Mar 12 08:54 OPENMP_DUP [username@login02 expanse-examples]$ To move to the directory, use the cd (change directory) [username@login02 expanse-examples]$ pwd /home/username/expanse-examples [username@login02 expanse-examples]$ cd OPENMP/ [username@login02 OPENMP]$ pwd /home/username/expanse-examples/OPENMP [username@login02 OPENMP]$ ls -al total 479 drwxr-xr-x 2 username abc123 8 Jul 17 20:26 . drwxr-xr-x 3 username abc123 4 Jul 17 20:32 .. -rwxr-xr-x 1 username abc123 728112 Jul 17 20:26 hello_openmp -rw-r--r-- 1 username abc123 984 Jul 17 20:26 hello_openmp.500005.expanse-27-01.out -rw-r--r-- 1 username abc123 247 Jul 17 20:26 hello_openmp.f90 -rw-r--r-- 1 username abc123 656 Jul 17 20:26 hello_openmp_shared.508392.expanse-11-01.out -rw-r--r-- 1 username abc123 310 Jul 17 20:26 openmp-slurm.sb -rw-r--r-- 1 username abc123 347 Jul 17 20:26 openmp-slurm-shared.sb You can sort the order of the file listings by date (or size or other fields -- see the man pages). The default file listing in alphabetic, to see the files in chronological order (or reverse): [username@login02 expanse-examples]$ ls -alt OPENMP/ total 479 drwxr-xr-x 4 username abc123 5 Jul 17 20:43 .. drwxrwxr-x 2 username abc123 8 Mar 12 08:54 . -rw-r--r-- 1 username abc123 656 Apr 22 2015 hello_openmp_shared.508392.expanse-11-01.out -rw-r--r-- 1 username abc123 347 Apr 22 2015 openmp-slurm-shared.sb -rw-r--r-- 1 username abc123 984 Apr 15 2015 hello_openmp.500005.expanse-27-01.out -rwxr-xr-x 1 username abc123 728112 Apr 15 2015 hello_openmp -rw-r--r-- 1 username abc123 247 Apr 15 2015 hello_openmp.f90 -rw-r--r-- 1 username abc123 310 Apr 15 2015 openmp-slurm.sb [username@login02 expanse-examples]$ ls -altr OPENMP/ total 479 -rw-r--r-- 1 username abc123 310 Apr 15 2015 openmp-slurm.sb -rw-r--r-- 1 username abc123 247 Apr 15 2015 hello_openmp.f90 -rwxr-xr-x 1 username abc123 728112 Apr 15 2015 hello_openmp -rw-r--r-- 1 username abc123 984 Apr 15 2015 hello_openmp.500005.expanse-27-01.out -rw-r--r-- 1 username abc123 347 Apr 22 2015 openmp-slurm-shared.sb -rw-r--r-- 1 username abc123 656 Apr 22 2015 hello_openmp_shared.508392.expanse-11-01.out drwxrwxr-x 2 username abc123 8 Mar 12 08:54 . drwxr-xr-x 4 username abc123 5 Jul 17 20:43 .. Back to Top","title":"Copying directories"},{"location":"basic_skills/basic_linux_skills_expanse/#manipulating-files","text":"This section will show you more ways to manipulate files: copying, listing, deleting and renaming, and examining contents In the section above, we created files using the touch commmand, which will create a file with no contents. First we'll return to the first testdir using the full directory path : [username@login01 testdir2]$ cd /home/username/testdir [username@login02 testdir]$ touch myfile1.txt [username@login02 testdir]$ touch myfile2.txt [username@login02 testdir]$ ls -l total 21 drwxr-xr-x 2 username abc123 4 Jul 17 20:53 . drwxr-x--- 9 username abc123 25 Jul 17 20:49 .. -rw-r--r-- 1 username abc123 0 Jul 17 20:53 myfile1.txt -rw-r--r-- 1 username abc123 0 Jul 17 20:53 myfile2.txt To copy a file from another directory to the current directory, use the full path. In this example, we will copy the filelisting.txt file from the directory above, using the .. (\\\"dot-dot\\\") variable for the directory location: [username@login02 testdir]$ cp ../filelisting.txt . [username@login02 testdir]$ ls -l total 11 -rw-r--r-- 1 username abc123 322 Jul 17 21:09 filelisting.txt -rw-r--r-- 1 username abc123 0 Jul 17 20:53 myfile1.txt -rw-r--r-- 1 username abc123 0 Jul 17 20:53 myfile2.txt To rename a file, use the mv (move) command: [username@login02 testdir]$ mv myfile2.txt newfile.txt [username@login02 testdir]$ ls -l total 11 -rw-r--r-- 1 username abc123 1543 Jul 17 21:09 filelisting.txt -rw-r--r-- 1 username abc123 0 Jul 17 20:53 myfile1.txt -rw-r--r-- 1 username abc123 0 Jul 17 20:53 newfile.txt To delete a file, use the rm (remove command): [username@login02 testdir]$ rm myfile1.txt [username@login02 testdir]$ ls -l total 10 -rw-r--r-- 1 username abc123 1543 Jul 17 21:09 filelisting.txt -rw-r--r-- 1 username abc123 0 Jul 17 20:53 newfile.txt -rw-r--r-- 1 username use300 1410 Jan 18 00:00 sdsc.txt You can examine the contents of a file by using several Linux commands. First, we'll create a small file for testing: [username@login01 testdir]$ ls -al > dirlist.txt [username@login01 testdir]$ ls -al total 88 drwxr-xr-x 2 username use300 5 Jan 17 23:49 . drwxr-x--- 51 username use300 93 Jan 17 23:46 .. -rw-r--r-- 1 username use300 284 Jan 17 23:49 dirlist.txt -rw-r--r-- 1 username use300 322 Jan 17 23:42 filelisting.txt -rw-r--r-- 1 username use300 0 Jan 17 23:42 newfile.txt -rw-r--r-- 1 username use300 1410 Jan 18 00:00 sdsc.txt To print out the contents of the entire file, use the cat command (cat - concatenate files and print on the standard output): [username@login01 testdir]$ cat sdsc.txt The San Diego Supercomputer Center (SDSC) was established as one of the nation\u2019s first supercomputer centers under a cooperative agreement by the National Science Foundation (NSF) in collaboration with UC San Diego and General Atomics (GA) Technologies. SDSC first opened its doors on November 14, 1985. For more than 35 years, it has grown and stewarded its national reputation as a pioneer and leader in high-performance and data-intensive computing and cyberinfrastructure. Located on the campus of UC San Diego, SDSC provides resources, services and expertise to the national research community including industry and academia. Cyberinfrastructure refers to an accessible, integrated network of computer-based resources and expertise, focused on accelerating scientific inquiry and discovery. With Voyager and Expanse, SDSC\u2019s latest supercomputing resources, the center supports hundreds of multidisciplinary programs spanning a wide range of science themes\u2014from earth sciences and biology to astrophysics, bioinformatics and health IT. SDSC participates in Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) and was a partner in eXtreme Science and Engineering Discovery Environment (XSEDE), two of the most advanced collections of integrated digital resources and services in the world. For general inquiries and comments: info@sdsc.edu SDSC website: www.sdsc.edu The contents of a file may be too large to see at one time, to see how large it is we can run the wc , word count command: [username@login01 testdir]$ wc ../README.md 644 4531 30519 ../README.md The output says that README file has 644 lines, 4531 words, and 30519 characters. We might want to look at the beginning (head) or end (tail): [username@login01 testdir]$ head -n 1 sdsc.txt The San Diego Supercomputer Center (SDSC) was established as one of the nation\u2019s first supercomputer centers under a cooperative agreement by the National Science Foundation (NSF) in collaboration with UC San Diego and General Atomics (GA) Technologies. SDSC first opened its doors on November 14, 1985. [username@login01 testdir]$ [username@login01 testdir]$ tail -n 2 sdsc.txt For general inquiries and comments: info@sdsc.edu SDSC website: www.sdsc.edu You can reorder the contents of a file using the sort command: [mthomas@login01 testdir]$ sort sdsc.txt Cyberinfrastructure refers to an accessible, integrated network of computer-based resources and expertise, focused on accelerating scientific inquiry and discovery. With Voyager and Expanse, SDSC\u2019s latest supercomputing resources, the center supports hundreds of multidisciplinary programs spanning a wide range of science themes\u2014from earth sciences and biology to astrophysics, bioinformatics and health IT. For general inquiries and comments: info@sdsc.edu For more than 35 years, it has grown and stewarded its national reputation as a pioneer and leader in high-performance and data-intensive computing and cyberinfrastructure. Located on the campus of UC San Diego, SDSC provides resources, services and expertise to the national research community including industry and academia. SDSC participates in Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) and was a partner in eXtreme Science and Engineering Discovery Environment (XSEDE), two of the most advanced collections of integrated digital resources and services in the world. SDSC website: www.sdsc.edu The San Diego Supercomputer Center (SDSC) was established as one of the nation\u2019s first supercomputer centers under a cooperative agreement by the National Science Foundation (NSF) in collaboration with UC San Diego and General Atomics (GA) Technologies. SDSC first opened its doors on November 14, 1985. The sort command is useful for sorting through a long file and reording data. To search a file for a string you can use the grep command: [mthomas@login01 testdir]$ grep -ni SDSC sdsc.txt 1:The San Diego Supercomputer Center (SDSC) was established as one of the nation\u2019s first supercomputer centers under a cooperative agreement by the National Science Foundation (NSF) in collaboration with UC San Diego and General Atomics (GA) Technologies. SDSC first opened its doors on November 14, 1985. 3:For more than 35 years, it has grown and stewarded its national reputation as a pioneer and leader in high-performance and data-intensive computing and cyberinfrastructure. Located on the campus of UC San Diego, SDSC provides resources, services and expertise to the national research community including industry and academia. 5:Cyberinfrastructure refers to an accessible, integrated network of computer-based resources and expertise, focused on accelerating scientific inquiry and discovery. With Voyager and Expanse, SDSC\u2019s latest supercomputing resources, the center supports hundreds of multidisciplinary programs spanning a wide range of science themes\u2014from earth sciences and biology to astrophysics, bioinformatics and health IT. 7:SDSC participates in Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) and was a partner in eXtreme Science and Engineering Discovery Environment (XSEDE), two of the most advanced collections of integrated digital resources and services in the world. 9:For general inquiries and comments: info@sdsc.edu 10:SDSC website: www.sdsc.edu In the command above, we used the grep -ni argument to get the line number, and to ignore case. Back to Top","title":"Manipulating Files"},{"location":"basic_skills/basic_linux_skills_expanse/#permissions","text":"In the section we will look breifly at how to set file permissions. Before you can change the file permissions, you need to own it or have permission as a 2023 member. For a more detailed tutorial, see http://www.nersc.gov/users/storage-and-file-systems/unix-file-permissions/. Permissions are written in the first column, with fields that specify whether or not the file is a directory ( d ), what the read/write/execution permissions ( rwx ) for the files are for users and groups. Using the example below: [username@login02 OPENMP]$ ls -l hello_openmp total 479 drwxr-xr-x 2 username abc123 2 Jul 17 21:53 direxample -rwxr-xr-x 1 username abc123 728112 Apr 15 2015 hello_openmp -rw-r--r-- 1 username abc123 247 Apr 15 2015 hello_openmp.f90 The order of the markers are grouped into 4 fields: - rwx r-x r-x Field 1 == directory, a d or - means directory or not a directory Field 2 == owner permissions 'rwx' means the owner can read, write, and exectute Field 3 == 2023 permissions 'rwx' means the owner can read and exectute, but not modify Field 4 == other/world permissions 'r-x' means the others can read and exectute, but not modiry To change the file access permissions, use the chmod command. In the example below, only user username has permission to edit ( rw- ) the files, members of the 2023 abc123 and others have read only permission ( -- ). There are several ways to modify permissions, we will use the binary representation where the rwx status represents a binary number 2^n, where n is the position of the permission starting from the right. For example: r-- = 2^2 + 0 + 0 = 4 + 0 + 0 = 4 rw- = 2^2 + 2^1 + 0 = 4 + 2 + 0 = 6 r-x = 2^2 + 0 + 2^0 = 4 + 0 + 1 = 5 rwx = 2^2 + 2^1 + 2^0 = 4 + 2 + 1 = 7 In the example below, we will set read and write permissions to the owner and the group, and limit the other/world 2023 to read only: [username@login02 OPENMP]$ ls -l total 479 drwxr-xr-x 2 username abc123 2 Jul 17 21:53 direxample -rwxr-xr-x 1 username abc123 728112 Apr 15 2015 hello_openmp -rw-r--r-- 1 username abc123 984 Apr 15 2015 hello_openmp.500005.expanse-27-01.out -rw-r--r-- 1 username abc123 247 Apr 15 2015 hello_openmp.f90 -rw-r--r-- 1 username abc123 656 Apr 22 2015 hello_openmp_shared.508392.expanse-11-01.out -rw-r--r-- 1 username abc123 310 Apr 15 2015 openmp-slurm.sb -rw-r--r-- 1 username abc123 347 Apr 22 2015 openmp-slurm-shared.sb [username@login02 OPENMP]$ chmod 660 * [username@login02 OPENMP]$ ls -l total 460 drwxr-xr-x 2 username abc123 2 Jul 17 21:53 direxample -rw-rw-r-- 1 username abc123 728112 Apr 15 2015 hello_openmp -rw-rw---- 1 username abc123 984 Apr 15 2015 hello_openmp.500005.expanse-27-01.out -rw-rw---- 1 username abc123 247 Apr 15 2015 hello_openmp.f90 -rw-rw---- 1 username abc123 656 Apr 22 2015 hello_openmp_shared.508392.expanse-11-01.out -rw-rw---- 1 username abc123 310 Apr 15 2015 openmp-slurm.sb -rw-rw---- 1 username abc123 347 Apr 22 2015 openmp-slurm-shared.sb In the example above, we use the star wildcard, \\\" * \\\" to represent all the files in the directory (See the section on wildcards below). We can use the wildcard to change the group of some of the files. For example, to change the 2023 of only the *.out files: [username@login02 OPENMP]$ groups abc123 pet heart scicom-docs grdclus webwrt ... [username@login02 OPENMP]$ chgrp heart *.out [username@login02 OPENMP]$ ls -l total 460 drwxr-xr-x 2 username abc123 2 Jul 17 21:53 direxample -rw-rw-r-- 1 username abc123 728112 Apr 15 2015 hello_openmp -rw-rw---- 1 username heart 984 Apr 15 2015 hello_openmp.500005.expanse-27-01.out -rw-rw---- 1 username abc123 247 Apr 15 2015 hello_openmp.f90 -rw-rw---- 1 username heart 656 Apr 22 2015 hello_openmp_shared.508392.expanse-11-01.out -rw-rw---- 1 username abc123 310 Apr 15 2015 openmp-slurm.sb -rw-rw---- 1 username abc123 347 Apr 22 2015 openmp-slurm-shared.sb Back to Top","title":"Permissions"},{"location":"basic_skills/basic_linux_skills_expanse/#wildcards","text":"\\\"A wildcard is a character that can be used as a substitute for any of a class of characters in a search, thereby greatly increasing the flexibility and efficiency of searches.\" The wildcards are very powerful, and there is not room in this document for all of them, so we recommend that you check out this site: http://www.linfo.org/wildcard.html for more information. In the example below, we use the star wildcard to list all files ending in .out [username@login02 OPENMP]$ ls -al *.out -rw-rw---- 1 username heart 984 Apr 15 2015 hello_openmp.500005.expanse-27-01.out -rw-rw---- 1 username heart 656 Apr 22 2015 hello_openmp_shared.508392.expanse-11-01.out Back to Top","title":"Wildcards"},{"location":"basic_skills/basic_linux_skills_expanse/#other-utilities-to-learj","text":"grep, sort, tar, gzip ,and pigz","title":"Other Utilities to Learj:"},{"location":"basic_skills/interactive_computing/","text":"Interactive Computing on the Expanse Cluster Interactive HPC systems allow real-time user inputs in order to facilitate code development, real-time data exploration, and visualizations. An interactive job (also referred as interactive session) will provide you with a shell on a compute node in which you can launch your jobs. In this tutorial we show you how to obtain interactive nodes on SDSC HPC systems Note For more details on using Expanse, see the user guide: https://www.sdsc.edu/support/user_guides/expanse.html Contents * Obtaining Interactive Nodes * Request an interactive CPU node from the command line: * Obtain interactive shared GPU node on Expanse Obtaining Interactive Nodes There are two ways to obtain interactive nodes: 1. via the command line 2. via a batch script Log onto expanse.sdsc.edu ssh -Y -l <username> expanse.sdsc.edu To learn more about logging onto the Expase cluster, see these instructions: https://github.com/sdsc-hpc-training-org/hpc-security/blob/master/connecting-to-hpc-systems/connect-to-expanse.md Back to Top Request an interactive CPU node from the command line: You can request an interactive session using the The SLURM launch srun command. The following example will request one regular compute node, 4 cores, in the debug partition for 30 minutes. srun --partition=debug --pty --account=<<project>> --nodes=1 --ntasks-per-node=4 --mem=8G -t 00:30:00 --wait=0 --export=ALL /bin/bash Wait for your node to be allocated. This may take a while, depending on how busy the system is. When you have your node, you will get a message like this: srun: job 13469789 queued and waiting for resources srun: job 13469789 has been allocated resources [user@exp-9-55 ~]$ Notice that you are logged onto a different node than the login node. Run some commands to learn about the node: ``` [user@exp-9-55 ~]$ hostname exp-9-55.sdsc.edu [user@exp-9-55 ~]$ who [user@exp-9-55 ~]$ whoami user [user@exp-9-55 ~]$ [user@exp-9-55 ~]$ top - 21:37:07 up 15 days, 15:58, 0 users, load average: 0.03, 0.06, 0.05 Tasks: 620 total, 1 running, 619 sleeping, 0 stopped, 0 zombie %Cpu(s): 0.0 us, 0.0 sy, 0.0 ni, 99.9 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 13165400+total, 12897894+free, 1933784 used, 741272 buff/cache KiB Swap: 0 total, 0 free, 0 used. 12866142+avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 17259 root 20 0 22408 15684 2476 S 0.7 0.0 30:25.44 serf 6380 root 20 0 0 0 0 S 0.3 0.0 0:13.09 jbd2/md1-8 1 root 20 0 192860 4816 1604 S 0.0 0.0 2:30.00 systemd 2 root 20 0 0 0 0 S 0.0 0.0 0:02.72 kthreadd 3 root 20 0 0 0 0 S 0.0 0.0 0:07.61 ksoftirqd/0 8 root rt 0 0 0 0 S 0.0 0.0 0:01.05 migration/0 9 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcu_bh 10 root 20 0 0 0 0 S 0.0 0.0 4:00.35 rcu_sched * At this point, you can edit, compile, run code, including MPI, OpenMP, or jupyter notebooks. * For an example of running a notebook, see the the following tutorial: * https://github.com/sdsc-hpc-training/basic_skills/tree/master/how_to_run_notebooks_on_expanse * For a collection of test notebooks, see: https://github.com/sdsc-hpc-training-org/notebook-examples-expanse [Back to Top](#top) <hr> ## Obtain interactive shared GPU node on SDSC Expanse <a name=\"interactive-gpu-command-line\"></a> * This works the same way, but you need to access the GPU nodes * The SLURM launch command below will obtain an interactive node with access to 1 K80 GPU on the shared GPU nodes for 3h. You can also execute this command directly on the command line: srun --partition=gpu-shared --reservation=gputraining --nodes=1 --ntasks-per-node=6 --gres=gpu:k80:1 -t 03:00:00 --pty --wait=0 /bin/bash * It may take some time to get the interactive node. * Load the CUDA and PGI compiler modules module purge module load gnutools module load cuda module load pgi * If you get a license error when executing the PGI compilers, execute the following: export LM_LICENSE_FILE=40200@elprado.sdsc.edu:$LM_LICENSE_FILE ``` Back to Top","title":"Interactive Computing on the Expanse Cluster"},{"location":"basic_skills/interactive_computing/#interactive-computing-on-the-expanse-cluster","text":"Interactive HPC systems allow real-time user inputs in order to facilitate code development, real-time data exploration, and visualizations. An interactive job (also referred as interactive session) will provide you with a shell on a compute node in which you can launch your jobs. In this tutorial we show you how to obtain interactive nodes on SDSC HPC systems Note For more details on using Expanse, see the user guide: https://www.sdsc.edu/support/user_guides/expanse.html Contents * Obtaining Interactive Nodes * Request an interactive CPU node from the command line: * Obtain interactive shared GPU node on Expanse","title":"Interactive Computing on the Expanse Cluster"},{"location":"basic_skills/interactive_computing/#obtaining-interactive-nodes","text":"There are two ways to obtain interactive nodes: 1. via the command line 2. via a batch script","title":"Obtaining Interactive Nodes"},{"location":"basic_skills/interactive_computing/#log-onto-expansesdscedu","text":"ssh -Y -l <username> expanse.sdsc.edu To learn more about logging onto the Expase cluster, see these instructions: https://github.com/sdsc-hpc-training-org/hpc-security/blob/master/connecting-to-hpc-systems/connect-to-expanse.md Back to Top","title":"Log onto expanse.sdsc.edu"},{"location":"basic_skills/interactive_computing/#request-an-interactive-cpu-node-from-the-command-line","text":"You can request an interactive session using the The SLURM launch srun command. The following example will request one regular compute node, 4 cores, in the debug partition for 30 minutes. srun --partition=debug --pty --account=<<project>> --nodes=1 --ntasks-per-node=4 --mem=8G -t 00:30:00 --wait=0 --export=ALL /bin/bash Wait for your node to be allocated. This may take a while, depending on how busy the system is. When you have your node, you will get a message like this: srun: job 13469789 queued and waiting for resources srun: job 13469789 has been allocated resources [user@exp-9-55 ~]$ Notice that you are logged onto a different node than the login node. Run some commands to learn about the node: ``` [user@exp-9-55 ~]$ hostname exp-9-55.sdsc.edu [user@exp-9-55 ~]$ who [user@exp-9-55 ~]$ whoami user [user@exp-9-55 ~]$ [user@exp-9-55 ~]$ top - 21:37:07 up 15 days, 15:58, 0 users, load average: 0.03, 0.06, 0.05 Tasks: 620 total, 1 running, 619 sleeping, 0 stopped, 0 zombie %Cpu(s): 0.0 us, 0.0 sy, 0.0 ni, 99.9 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 13165400+total, 12897894+free, 1933784 used, 741272 buff/cache KiB Swap: 0 total, 0 free, 0 used. 12866142+avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 17259 root 20 0 22408 15684 2476 S 0.7 0.0 30:25.44 serf 6380 root 20 0 0 0 0 S 0.3 0.0 0:13.09 jbd2/md1-8 1 root 20 0 192860 4816 1604 S 0.0 0.0 2:30.00 systemd 2 root 20 0 0 0 0 S 0.0 0.0 0:02.72 kthreadd 3 root 20 0 0 0 0 S 0.0 0.0 0:07.61 ksoftirqd/0 8 root rt 0 0 0 0 S 0.0 0.0 0:01.05 migration/0 9 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcu_bh 10 root 20 0 0 0 0 S 0.0 0.0 4:00.35 rcu_sched * At this point, you can edit, compile, run code, including MPI, OpenMP, or jupyter notebooks. * For an example of running a notebook, see the the following tutorial: * https://github.com/sdsc-hpc-training/basic_skills/tree/master/how_to_run_notebooks_on_expanse * For a collection of test notebooks, see: https://github.com/sdsc-hpc-training-org/notebook-examples-expanse [Back to Top](#top) <hr> ## Obtain interactive shared GPU node on SDSC Expanse <a name=\"interactive-gpu-command-line\"></a> * This works the same way, but you need to access the GPU nodes * The SLURM launch command below will obtain an interactive node with access to 1 K80 GPU on the shared GPU nodes for 3h. You can also execute this command directly on the command line: srun --partition=gpu-shared --reservation=gputraining --nodes=1 --ntasks-per-node=6 --gres=gpu:k80:1 -t 03:00:00 --pty --wait=0 /bin/bash * It may take some time to get the interactive node. * Load the CUDA and PGI compiler modules module purge module load gnutools module load cuda module load pgi * If you get a license error when executing the PGI compilers, execute the following: export LM_LICENSE_FILE=40200@elprado.sdsc.edu:$LM_LICENSE_FILE ``` Back to Top","title":"Request an interactive CPU node from the command line:"},{"location":"basic_skills/running_notebooks_on_expanse/","text":"Using Jupyter Notebooks on the SDSC Expanse Cluster By Mary Thomas, SDSC Updated: April 24, 2024 Contents Using the galyleo script Running CONDA Environments and Jupyter Notebook on Expanse Notebook Examples This quick-start guide will show you how to run Jupyter notebooks on Expanse using the Satellite Reverse Proxy Service . Satellite is a prototype system that will allow users to launch secure (HTTPS) Jupyter Services on any Expanse compute node using a simple bash script called galyleo or start_notebook . The notebooks will be made available to the user outside of the cluster firewall using a secure HTTPS connection between the external users web browser and the reverse proxy server. Using the galyleo script galyleo is a shell utility to help you launch Jupyter notebooks on high-performance computing (HPC) systems in a simple, secure way. It works with the Satellite reverse proxy service and a batch job scheduler like Slurm to provide each Jupyter notebook server you start with its own one-time, token-authenticated HTTPS connection between the compute resources of the HPC system the notebook server is running on and your web browser. This HTTPS-secured connection affords both privacy and integrity to the data exchanged between the notebook server and your browser, helping protect you and your work against network eavesdropping and data tampering. For details on using galyleo, see: https://github.com/mkandes/galyleo Running CONDA Environments and Jupyter Notebook on Expanse see the CIML Summer Institute material: Session 3.3 CONDA Environments and Jupyter Notebook on Expanse: Scalable & Reproducible Data Exploration and ML Example Notebooks Clone the notebook example repository, or use one you have already created * SDSC HPC Notebook Examples git clone https://github.com/sdsc-hpc-training-org/notebook-examples-expanse.git Launch the secure notebook Using the Galyleo command","title":"Using Jupyter Notebooks on the SDSC Expanse Cluster"},{"location":"basic_skills/running_notebooks_on_expanse/#using-jupyter-notebooks-on-the-sdsc-expanse-cluster","text":"By Mary Thomas, SDSC Updated: April 24, 2024","title":"Using Jupyter Notebooks on the SDSC Expanse Cluster"},{"location":"basic_skills/running_notebooks_on_expanse/#contents","text":"Using the galyleo script Running CONDA Environments and Jupyter Notebook on Expanse Notebook Examples This quick-start guide will show you how to run Jupyter notebooks on Expanse using the Satellite Reverse Proxy Service . Satellite is a prototype system that will allow users to launch secure (HTTPS) Jupyter Services on any Expanse compute node using a simple bash script called galyleo or start_notebook . The notebooks will be made available to the user outside of the cluster firewall using a secure HTTPS connection between the external users web browser and the reverse proxy server.","title":"Contents"},{"location":"basic_skills/running_notebooks_on_expanse/#using-the-galyleo-script","text":"galyleo is a shell utility to help you launch Jupyter notebooks on high-performance computing (HPC) systems in a simple, secure way. It works with the Satellite reverse proxy service and a batch job scheduler like Slurm to provide each Jupyter notebook server you start with its own one-time, token-authenticated HTTPS connection between the compute resources of the HPC system the notebook server is running on and your web browser. This HTTPS-secured connection affords both privacy and integrity to the data exchanged between the notebook server and your browser, helping protect you and your work against network eavesdropping and data tampering. For details on using galyleo, see: https://github.com/mkandes/galyleo","title":"Using the galyleo script"},{"location":"basic_skills/running_notebooks_on_expanse/#running-conda-environments-and-jupyter-notebook-on-expanse","text":"see the CIML Summer Institute material: Session 3.3 CONDA Environments and Jupyter Notebook on Expanse: Scalable & Reproducible Data Exploration and ML","title":"Running CONDA Environments and Jupyter Notebook on Expanse"},{"location":"basic_skills/running_notebooks_on_expanse/#example-notebooks","text":"Clone the notebook example repository, or use one you have already created * SDSC HPC Notebook Examples git clone https://github.com/sdsc-hpc-training-org/notebook-examples-expanse.git Launch the secure notebook Using the Galyleo command","title":"Example Notebooks"},{"location":"basic_skills/using_github/","text":"Using GitHub at SDSC Github repositories can be accessed and updated on all SDSC HPC systems. Make sure your local system or laptop can run git. Suggested On-line Tutorials https://swcarpentry.github.io/git-novice/ Contents:: * Create Git Account * Install Git on Local System * Install Git on Linux * Install Git on Mac * Install Git on Windows * Clone the Git repository * Checkout a branch Create a GitHub Account: See: see https://github.com/join If you do not currently have a GitHub account, you must obtain one prior to attending the SDSC training activities. Working with git and GitHub will be covered in both introductory and advanced training sessions on these tools. Install Git on Local System To learn the basics of setting up GitHub, see this guide: https://help.github.com/en/articles/set-up-git . Install git on Linux: If you want to install git on a Linux-based operating system, you should be able to do so via your operating system's standard package management tool. For example, on any RPM-based Linux distribution, such as Fedora, RHEL, or CentOS, you can use dnf : $ sudo dnf install git-all Or on any Debian-based distribution, such as Ubuntu, try apt : $ sudo apt install git-all Installing git on Mac OS X: There are several ways to install git on your Mac. However, probably the easiest way is to install the Xcode Command Line Tools, which you should be able to do by simply tying to run git from your Terminal : $ git --version If it's not already installed, you should be prompted to install it. If the above option does not work or you need a more up-to-date version of git , you can always install it via a binary installer maintained by the git team, which is available for download at: https://git-scm.com/download/mac . The download should start immediately. Installing git on Windows: There are also several ways to install git under Windows. The official binary executable is available for download on the git website is here: https://git-scm.com/download/win If you've chosen to work on the Windows Subsystem for Linux follow the installation directions for Linux given above. Cloning a Repository: GitHub supports two ways to clone a repository: (1) Anonymous HTTPS cloning (no account required); (2) Authenticated method using a SSH, which requires authentication to a GitHub account associated with the repository. As an example, we will download the SDSC Jupyter \"Notebook Examples\" Repository: https://github.com/sdsc-hpc-training-org/notebook-examples Anonymous HTTPS Method: click on the option on the GitHub repo To clone the Notebook repository: Open a terminal window on your laptop. Optionally, create a directory to save the repo; cd into that directory In your browser, open the link to the repository web page: https://github.com/sdsc-hpc-training-org/notebook-examples Click on the green \"Clone or Download\" and select the \"Clone with HTTPS\" option copy the repository link in the box In your terminal window, type the following command $ git clone https://github.com/sdsc-hpc-training-org/notebook-examples.git The repository should start downloading in the directory from which you ran the clone command. Authenticated cloning: This method requires that you create a GitHub account and then you must be added to the repository project team. Checkout a Branch: Make sure you have the main repository cloned locally. Then change to the root of the local repository. (base) [username@login01 ~]$ git clone https://github.com/sdsc-hpc-training-org/basic_skills.git Cloning into 'basic_skills'... remote: Enumerating objects: 330, done. remote: Counting objects: 100% (330/330), done. remote: Compressing objects: 100% (240/240), done. remote: Total 330 (delta 152), reused 160 (delta 58), pack-reused 0 Receiving objects: 100% (330/330), 4.10 MiB | 12.21 MiB/s, done. Resolving deltas: 100% (152/152), done. (base) [username@login01 ~]$ cd basic_skills/ List all available branches: (base) [username@login01 basic_skills]$ git branch -a * master remotes/origin/HEAD -> origin/master remotes/origin/basic_skills_branch remotes/origin/master Notice that it lists both the branches that are local and the remote branches on Bitbucket. Using the list as reference, choose the branch you want to checkout. In this example, we want basic_skills_branch. (base) [username@login01 basic_skills]$ git checkout basic_skills_branch Branch 'basic_skills_branch' set up to track remote branch 'basic_skills_branch' from 'origin'. Switched to a new branch 'basic_skills_branch' (base) [username@login01 basic_skills]$ (base) [username@login01 basic_skills]$ Verify that you have checkout the right branch: (base) [username@login01 basic_skills]$ (base) [username@login01 basic_skills]$ git branch * basic_skills_branch master (base) [username@login01 basic_skills]$ At this point, all changes made will affect the branch, not the master","title":"Using GitHub at SDSC"},{"location":"basic_skills/using_github/#using-github-at-sdsc","text":"Github repositories can be accessed and updated on all SDSC HPC systems. Make sure your local system or laptop can run git.","title":"Using GitHub at SDSC"},{"location":"basic_skills/using_github/#suggested-on-line-tutorials","text":"https://swcarpentry.github.io/git-novice/ Contents:: * Create Git Account * Install Git on Local System * Install Git on Linux * Install Git on Mac * Install Git on Windows * Clone the Git repository * Checkout a branch","title":"Suggested On-line Tutorials"},{"location":"basic_skills/using_github/#create-a-github-account","text":"See: see https://github.com/join If you do not currently have a GitHub account, you must obtain one prior to attending the SDSC training activities. Working with git and GitHub will be covered in both introductory and advanced training sessions on these tools.","title":"Create a GitHub Account:"},{"location":"basic_skills/using_github/#install-git-on-local-system","text":"To learn the basics of setting up GitHub, see this guide: https://help.github.com/en/articles/set-up-git .","title":"Install Git on Local System"},{"location":"basic_skills/using_github/#install-git-on-linux","text":"If you want to install git on a Linux-based operating system, you should be able to do so via your operating system's standard package management tool. For example, on any RPM-based Linux distribution, such as Fedora, RHEL, or CentOS, you can use dnf : $ sudo dnf install git-all Or on any Debian-based distribution, such as Ubuntu, try apt : $ sudo apt install git-all","title":"Install git on Linux:"},{"location":"basic_skills/using_github/#installing-git-on-mac-os-x","text":"There are several ways to install git on your Mac. However, probably the easiest way is to install the Xcode Command Line Tools, which you should be able to do by simply tying to run git from your Terminal : $ git --version If it's not already installed, you should be prompted to install it. If the above option does not work or you need a more up-to-date version of git , you can always install it via a binary installer maintained by the git team, which is available for download at: https://git-scm.com/download/mac . The download should start immediately.","title":"Installing git on Mac OS X:"},{"location":"basic_skills/using_github/#installing-git-on-windows","text":"There are also several ways to install git under Windows. The official binary executable is available for download on the git website is here: https://git-scm.com/download/win If you've chosen to work on the Windows Subsystem for Linux follow the installation directions for Linux given above.","title":"Installing git on Windows:"},{"location":"basic_skills/using_github/#cloning-a-repository","text":"GitHub supports two ways to clone a repository: (1) Anonymous HTTPS cloning (no account required); (2) Authenticated method using a SSH, which requires authentication to a GitHub account associated with the repository. As an example, we will download the SDSC Jupyter \"Notebook Examples\" Repository: https://github.com/sdsc-hpc-training-org/notebook-examples Anonymous HTTPS Method: click on the option on the GitHub repo To clone the Notebook repository: Open a terminal window on your laptop. Optionally, create a directory to save the repo; cd into that directory In your browser, open the link to the repository web page: https://github.com/sdsc-hpc-training-org/notebook-examples Click on the green \"Clone or Download\" and select the \"Clone with HTTPS\" option copy the repository link in the box In your terminal window, type the following command $ git clone https://github.com/sdsc-hpc-training-org/notebook-examples.git The repository should start downloading in the directory from which you ran the clone command. Authenticated cloning: This method requires that you create a GitHub account and then you must be added to the repository project team.","title":"Cloning a Repository:"},{"location":"basic_skills/using_github/#checkout-a-branch","text":"Make sure you have the main repository cloned locally. Then change to the root of the local repository. (base) [username@login01 ~]$ git clone https://github.com/sdsc-hpc-training-org/basic_skills.git Cloning into 'basic_skills'... remote: Enumerating objects: 330, done. remote: Counting objects: 100% (330/330), done. remote: Compressing objects: 100% (240/240), done. remote: Total 330 (delta 152), reused 160 (delta 58), pack-reused 0 Receiving objects: 100% (330/330), 4.10 MiB | 12.21 MiB/s, done. Resolving deltas: 100% (152/152), done. (base) [username@login01 ~]$ cd basic_skills/ List all available branches: (base) [username@login01 basic_skills]$ git branch -a * master remotes/origin/HEAD -> origin/master remotes/origin/basic_skills_branch remotes/origin/master Notice that it lists both the branches that are local and the remote branches on Bitbucket. Using the list as reference, choose the branch you want to checkout. In this example, we want basic_skills_branch. (base) [username@login01 basic_skills]$ git checkout basic_skills_branch Branch 'basic_skills_branch' set up to track remote branch 'basic_skills_branch' from 'origin'. Switched to a new branch 'basic_skills_branch' (base) [username@login01 basic_skills]$ (base) [username@login01 basic_skills]$ Verify that you have checkout the right branch: (base) [username@login01 basic_skills]$ (base) [username@login01 basic_skills]$ git branch * basic_skills_branch master (base) [username@login01 basic_skills]$ At this point, all changes made will affect the branch, not the master","title":"Checkout a Branch:"},{"location":"galyleo/","text":"galyleo galyleo is a shell utility to help you launch Jupyter notebooks on high-performance computing (HPC) systems in a simple, secure way. It works with SDSC's Satellite reverse proxy service and a batch job scheduler like Slurm to provide each Jupyter notebook server you start with its own one-time, token-authenticated HTTPS connection between the compute resources of the HPC system the notebook server is running on and your web browser. This secure connection affords both privacy and integrity to the data exchanged between the notebook server and your browser, helping protect you and your work against network eavesdropping and data tampering. Quick Start Guide Command-line options Defining your software environment Environment modules Singularity containers Conda environments Debugging your session Additional Information Quick Start Guide galyleo is currently deployed on the following HPC systems at SDSC: Expanse Triton Shared Compute Cluster (TSCC) To use galyleo , you first need to prepend its install location to your PATH environment variable. This path is different for each HPC system at SDSC. On Expanse, use: export PATH=\"/cm/shared/apps/sdsc/galyleo:${PATH}\" On TSCC, there is now a software module available for loading galyleo into your environment. [mkandes@login1 ~]$ module load galyleo/0.7.4 [mkandes@login1 ~]$ module list Currently Loaded Modules: 1) shared 2) cpu/0.17.3 3) slurm/tscc/23.02.7 4) sdsc/1.0 5) DefaultModules 6) galyleo/0.7.4 [mkandes@login1 ~]$ which galyleo /cm/shared/apps/spack/0.17.3/cpu/opt/spack/linux-rocky9-cascadelake/gcc-11.2.0/galyleo-0.7.4/galyleo [mkandes@login1 ~]$ echo $PATH /cm/shared/apps/spack/0.17.3/cpu/opt/spack/linux-rocky9-cascadelake/gcc-11.2.0/galyleo-0.7.4:/tscc/nfs/home/mkandes/.local/bin:/tscc/nfs/home/mkandes/bin:/cm/shared/apps/sdsc/1.0/bin:/cm/shared/apps/sdsc/1.0/sbin:/cm/shared/apps/slurm/current/sbin:/cm/shared/apps/slurm/current/bin:/cm/shared/apps/spack/0.17.3/cpu/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin [mkandes@login1 ~]$ Once galyleo is in your PATH , you can then use its launch command to create a secure Jupyter notebook session. A number of command-line options will allow you to configure: the compute resources required to run your Jupyter notebook session; the type of Jupyter interface you want to use for the session and the location of the notebook working directory; and the software environment that contains the jupyter notebook server and the other software packages you want to work with during the session. For example, the following launch command will create a 30-minute JupyterLab session on two CPU-cores with 4 GB of memory on one of Expanse's shared AMD compute nodes using the base anaconda3 distribution available in its default cpu software module environment. galyleo launch --account abc123 --partition shared --cpus 2 --memory 4 --time-limit 00:30:00 --env-modules cpu/0.17.3b,anaconda3/2021.05 When the launch command completes successfully, you will be issued a unique HTTPS URL generated for your secure Jupyter notebook session. https://wages-astonish-recapture.expanse-user-content.sdsc.edu?token=1abe04ac1703ca623e4e907cc37678ae Copy and paste this HTTPS URL into your web browser. Your Jupyter notebook session will begin once the requested compute resources are allocated to your job by the scheduler. Command-line options A list of the most commonly used command-line options for the launch command are described below. Scheduler options: -A, --account : charge the compute resources required by this job to the specified account or allocation project id -p, --partition : select the resource partition or queue the job should be submitted to -c, --cpus : number of cpus to request for the job -m, --memory : amount of memory (in GB) required for the job -g, --gpus : number of GPUs required for the job -t, --time-limit : set a maximum runtime (in HH:MM:SS) for the job -C, --constraint : apply a feature constraint to specify the type of compute node required for the job Jupyter options: - -i, --interface : select the user interface for the Jupyter notebook session; the only options are lab or notebook or voila - -d, --notebook-dir : path to the working directory where the Jupyter notebook session will start; default value is your $HOME directory Software environment options: - -e, --env-modules : comma-separated list of environment modules that will be loaded to create the software environment for the Jupyter notebook session - -s, --sif : path to a Singularity container image file that will be run to create the software environment for the Jupyter notebook session - -B, --bind : comma-separated list of user-defined bind paths to be mounted within a Singularity container - --nv : enable NVIDIA GPU support when running a Singularity container - --conda-env : name of a conda environment to activate to create the software environment for the Jupyter notebook session - --conda-yml : path to an environment.yml file - --mamba : use mamba instead of miniconda to create your conda environment from an environment.yml file. - --cache : cache your conda environment created from an environment.yml file using conda-pack; a cached environment will be unpacked and reused if the environment.yml file does not change Defining your software environment After you specify the compute resources required for your Jupyter notebook session using the Scheduler options outlined above, the next most important set of command-line options for the launch command are those that help you define the software environment. Listed in the Software environment options section above, these command-line options are discussed in detail in the next few subsections below. Environment modules Most HPC systems use a software module system like Lmod or Environment Modules to provide you with a convenient way to dynamically load pre-installed software applications, libraries, and other packages into your shell's environment. If you need to module load any software to create the environment for your Jupyter notebook session, you can do so by including them as a comma-separated list to the --env-modules option in your launch command. Each module included in the list will be loaded prior to starting jupyter . In some cases, the --env-modules command-line option may be the only one you need to define your software environment. For example, if you have a standard Python-based data science workflow that you want run on Expanse, then you might only need to load one of the Anaconda distributions available in its software module environment. galyleo launch --account abc123 --partition shared --cpus 2 --memory 4 --time-limit 00:30:00 --env-modules cpu/0.17.3b,anaconda3/2021.05 By default, each Anaconda distribution comes with over 250 of the most popular data science software packages pre-installed, including jupyter . Singularity containers Singularity containers bring operating system-level virtualization to scientific and high-performance computing, allowing you to package complete software environments --- including operating systems, software applications, libraries, and data --- in a simple, portable, and reproducible way, which can then be executed and run almost anywhere. If you have a Singularity container that you would like to run your Jupyter notebook session within, then you simply need to provide a path to the container with the --sif option in your launch command. This will start jupyter within the container using the singularity exec command. If necessary, you can also pass user-defined --bind mounts to the container and enable NVIDIA GPU support via the --nv flag. One of the most powerful features of Singularity is its ability to convert an existing Docker container to a Singularity container. So, even if you are not familiar with how to build your own Singularity container, you can always search public container registries like Docker Hub for an existing container that may help you get your work done. For example, let's say you need an R environment for your Jupyter notebook session. Why not try the latest r-notebook container from the Jupyter Docker Stacks project? To get started, you first use the singularity pull command to download and convert the Docker container to a Singularity container. singularity pull docker://jupyter/r-notebook:latest Once all of the layers of the Docker container have been downloaded and the container conversion process is complete, you can then launch your Jupyter notebook session with the newly built Singularity container. galyleo launch --account abc123 --cpus 2 --memory 4 --time-limit 00:30:00 --sif r-notebook_latest.sif On some systems like Expanse, you may need to load Singularity via the software module environment as well. galyleo launch --account abc123 --cpus 2 --memory 4 --time-limit 00:30:00 --env-modules singularitypro --sif r-notebook_latest.sif --bind /expanse,/scratch Here, the user-defined --bind mount option also enables access to both the /expanse network filesystems (e.g., /expanse/lustre ) and the local NVMe /scratch disk(s) available on each compute node from within the container. By default, only your $HOME directory is accessible from within the container. Singularity also provides native support for running containerized applications on NVIDIA GPUs. If you have a GPU-accelerated application you would like to run during your Jupyter notebook session, please make sure your container includes a CUDA-enabled version of the application that can utilize NVIDIA GPUs. NVIDIA distributes a number of GPU-optimized containers via their container registry . This includes containers for all of the most popular deep learning frameworks --- PyTorch , TensorFlow , and MXNet --- with jupyter pre-installed. Like the the containers available from DockerHub, you can pull these containers to the HPC system you are working on singularity pull docker://nvcr.io/nvidia/pytorch:21.07-py3 and then launch your Jupyter notebook session with galyleo . For example, you might want to run this PyTorch container on a single NVIDIA V100 GPU available in Expanse's gpu-shared partition. galyleo launch --account abc123 --partition gpu-shared --cpus 10 --memory 93 --gpus 1 --time-limit 00:30:00 --env-modules singularitypro --sif pytorch_21.07-py3.sif --bind /expanse,/scratch --nv Note, however, how you request GPU resources with galyleo may be different from one HPC system to another. For example, on Comet you must use the --gres command-line option on Comet to specify both the type and number of GPUs required for your Jupyter notebook session. The following launch command would create a session within the NVIDIA PyTorch container on a single P100 GPU available in Comet's gpu-shared partition. galyleo launch --account abc123 --partition gpu-shared --cpus 7 --gres gpu:p100:1 --time-limit 00:30:00 --sif pytorch_21.07-py3.sif --bind /oasis,/scratch --nv In contrast, on TSCC you'll never explicitly request a specific number of GPUs for your Jupyter notebook session. All GPUs on TSCC are currently allocated implicitly in proportion to the number of CPU-cores requested by a job and available on the type of GPU-accelerated compute node you expect it to run on. And if you would like to request your notebook session be scheduled on a certain type of GPU, then you must pass the type of GPU required listed in the pbsnodes properties via the --constraint command-line option. For example, the following launch command will schedule your session on one of the NVIDIA GeForce RTX ] 2080Ti GPUs available in the gpu-hotel queue on TSCC. galyleo launch --account abc123 --partition gpu-hotel --cpus 2 --constraint gpu2080ti --time-limit 00:30:00 --sif pytorch_21.07-py3.sif --bind /oasis --nv Whatever you do, whenever you're launching your Jupyter notebook session with galyleo from a Singularity container on compute resources with NVIDIA GPUs, don't forget the include the --nv flag. Conda environments Conda is an open-source software package and environment manager developed by Anaconda Inc. . Its ease of use, compatibility across multiple operating systems, and comprehensive support for both the Python and R software ecosystems has made it one of the most popular ways to build and maintain custom software environments in the data science and machine learning communities. And because of the constantly evolving software landscape in these spaces, which can involve quite complex software dependencies, conda is often the simplest way to get your custom Python or R software environment up and running on an HPC system. galyleo supports the use of conda environments to configure the software environment for your Jupyter notebook session. If you've already installed a conda distribution --- we recommend Miniconda --- and configured a custom conda environment within it, then you should only need to specify the name of the conda environment you want to activate for your notebook session with the --conda-env command-line option. For example, let's imagine you've already created a custom conda environment from the following environment.yml file. name: notebooks-sharing channels: - conda-forge - anaconda dependencies: - python=3.7 - jupyterlab=3 - pandas=1.2.4 - matplotlib=3.4.2 - seaborn=0.11.0 - scikit-learn=0.23.2 You should then be able to launch a 30-minute JupyterLab session on a four CPU-cores with 8 GB of memory on one of Expanse's shared AMD compute nodes by simply activating the notebooks-sharing environment. galyleo launch --account abc123 --partition shared --cpus 4 --memory 8 --time-limit 00:30:00 --conda-env notebooks-sharing Note, however, the use of the --conda-env command-line option here assumes you've already configured your ~/.bashrc file with the conda init command. If you have not done so (or choose not to do so), then you can also initialize any conda distribution in your launch command by providing the path to its conda.sh initialization script in the etc/profile.d directory via the --conda-init command-line option. galyleo launch --account abc123 --partition shared --cpus 4 --memory 8 --time-limit 00:30:00 --conda-env notebooks-sharing --conda-init miniconda3/etc/profile.d/conda.sh While creating your own custom software environment with conda may be convenient, it can also generate a high metadata load on the types of shared network filesystems you'll often find on an HPC system. At a minimum, if you install your conda distribution on a network filesystem, you can expect this to increase the installation time of software packages into your conda environment when compared to a local filesystem installation you may have done previously on your laptop. Under some circumstances, this metadata issue can lead to a serious degradation of the aggregate I/O performance across a filesystem, affecting the performance of all user jobs on the system. If you have not yet installed your conda environment on a shared filesystem (such as in your $HOME directory), galyleo now also allows you to dynamically create the environment at runtime from an environment.yml file. To use this feature, you simply need to provide the name of the environment.yml file with the --conda-yml command-line option. For example, if you wanted to start an Juoyter notebook session with the notebooks-sharing environment, you would use the following command: galyleo launch --account abc123 --partition shared --cpus 4 --memory 8 --time-limit 00:30:00 --conda-env notebooks-sharing --conda-yml environment.yml You can further improve the installation performance and reuse of these dynamically generated conda environments by using the new --mamba and --cache command-line options, which enables the use of Mamba to speed up software installs and saves the completed conda environment using conda-pack for future reuse, respectively. galyleo launch --account abc123 --partition shared --cpus 4 --memory 8 --time-limit 00:30:00 --conda-env notebooks-sharing --conda-yml environment.yml --mamba --cache Debugging your session If you experience a problem launching your Jupyter notebook session with galyleo , you may be able to debug the issue yourself by reviewing the batch job script generated by galyleo or the standard output/error file generated by the job itself. You can find these files stored in the hidden ~/.galyleo directory created in your HOME directory. Additional Information Expanse User Portal galyleo has been integrated with the Open OnDemand-based Expanse User Portal to help simplify launching Jupyter notebooks on Expanse. After logging into the portal, you can access this web-based interface to galyleo from the Interactive Apps tab in the toolbar across the top of your browser, then select Jupyter . Containers SDSC builds and maintains a number of custom Singularity containers for use on its HPC systems . Pre-built copies of many of these containers are made available from a central storage location on each HPC system. Please check the following locations for the latest containers. If you do not find the container you're looking for, please feel free to contact us and make a request for a container to be made available. On Expanse: - /cm/shared/apps/containers/singularity Status A work in progress. Contribute If you would like to contribute to the project, then please submit a pull request via GitHub. If you have a feature request or a problem to report, then please create a GitHub issue. Author Marty Kandes, Ph.D. Computational & Data Science Research Specialist High-Performance Computing User Services Group Data-Enabled Scientific Computing Division San Diego Supercomputer Center University of California, San Diego Version 0.7.6 Last Updated Monday, May 6th, 2024","title":"galyleo"},{"location":"galyleo/#galyleo","text":"galyleo is a shell utility to help you launch Jupyter notebooks on high-performance computing (HPC) systems in a simple, secure way. It works with SDSC's Satellite reverse proxy service and a batch job scheduler like Slurm to provide each Jupyter notebook server you start with its own one-time, token-authenticated HTTPS connection between the compute resources of the HPC system the notebook server is running on and your web browser. This secure connection affords both privacy and integrity to the data exchanged between the notebook server and your browser, helping protect you and your work against network eavesdropping and data tampering. Quick Start Guide Command-line options Defining your software environment Environment modules Singularity containers Conda environments Debugging your session Additional Information","title":"galyleo"},{"location":"galyleo/#quick-start-guide","text":"galyleo is currently deployed on the following HPC systems at SDSC: Expanse Triton Shared Compute Cluster (TSCC) To use galyleo , you first need to prepend its install location to your PATH environment variable. This path is different for each HPC system at SDSC. On Expanse, use: export PATH=\"/cm/shared/apps/sdsc/galyleo:${PATH}\" On TSCC, there is now a software module available for loading galyleo into your environment. [mkandes@login1 ~]$ module load galyleo/0.7.4 [mkandes@login1 ~]$ module list Currently Loaded Modules: 1) shared 2) cpu/0.17.3 3) slurm/tscc/23.02.7 4) sdsc/1.0 5) DefaultModules 6) galyleo/0.7.4 [mkandes@login1 ~]$ which galyleo /cm/shared/apps/spack/0.17.3/cpu/opt/spack/linux-rocky9-cascadelake/gcc-11.2.0/galyleo-0.7.4/galyleo [mkandes@login1 ~]$ echo $PATH /cm/shared/apps/spack/0.17.3/cpu/opt/spack/linux-rocky9-cascadelake/gcc-11.2.0/galyleo-0.7.4:/tscc/nfs/home/mkandes/.local/bin:/tscc/nfs/home/mkandes/bin:/cm/shared/apps/sdsc/1.0/bin:/cm/shared/apps/sdsc/1.0/sbin:/cm/shared/apps/slurm/current/sbin:/cm/shared/apps/slurm/current/bin:/cm/shared/apps/spack/0.17.3/cpu/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin [mkandes@login1 ~]$ Once galyleo is in your PATH , you can then use its launch command to create a secure Jupyter notebook session. A number of command-line options will allow you to configure: the compute resources required to run your Jupyter notebook session; the type of Jupyter interface you want to use for the session and the location of the notebook working directory; and the software environment that contains the jupyter notebook server and the other software packages you want to work with during the session. For example, the following launch command will create a 30-minute JupyterLab session on two CPU-cores with 4 GB of memory on one of Expanse's shared AMD compute nodes using the base anaconda3 distribution available in its default cpu software module environment. galyleo launch --account abc123 --partition shared --cpus 2 --memory 4 --time-limit 00:30:00 --env-modules cpu/0.17.3b,anaconda3/2021.05 When the launch command completes successfully, you will be issued a unique HTTPS URL generated for your secure Jupyter notebook session. https://wages-astonish-recapture.expanse-user-content.sdsc.edu?token=1abe04ac1703ca623e4e907cc37678ae Copy and paste this HTTPS URL into your web browser. Your Jupyter notebook session will begin once the requested compute resources are allocated to your job by the scheduler.","title":"Quick Start Guide"},{"location":"galyleo/#command-line-options","text":"A list of the most commonly used command-line options for the launch command are described below. Scheduler options: -A, --account : charge the compute resources required by this job to the specified account or allocation project id -p, --partition : select the resource partition or queue the job should be submitted to -c, --cpus : number of cpus to request for the job -m, --memory : amount of memory (in GB) required for the job -g, --gpus : number of GPUs required for the job -t, --time-limit : set a maximum runtime (in HH:MM:SS) for the job -C, --constraint : apply a feature constraint to specify the type of compute node required for the job Jupyter options: - -i, --interface : select the user interface for the Jupyter notebook session; the only options are lab or notebook or voila - -d, --notebook-dir : path to the working directory where the Jupyter notebook session will start; default value is your $HOME directory Software environment options: - -e, --env-modules : comma-separated list of environment modules that will be loaded to create the software environment for the Jupyter notebook session - -s, --sif : path to a Singularity container image file that will be run to create the software environment for the Jupyter notebook session - -B, --bind : comma-separated list of user-defined bind paths to be mounted within a Singularity container - --nv : enable NVIDIA GPU support when running a Singularity container - --conda-env : name of a conda environment to activate to create the software environment for the Jupyter notebook session - --conda-yml : path to an environment.yml file - --mamba : use mamba instead of miniconda to create your conda environment from an environment.yml file. - --cache : cache your conda environment created from an environment.yml file using conda-pack; a cached environment will be unpacked and reused if the environment.yml file does not change","title":"Command-line options"},{"location":"galyleo/#defining-your-software-environment","text":"After you specify the compute resources required for your Jupyter notebook session using the Scheduler options outlined above, the next most important set of command-line options for the launch command are those that help you define the software environment. Listed in the Software environment options section above, these command-line options are discussed in detail in the next few subsections below.","title":"Defining your software environment"},{"location":"galyleo/#environment-modules","text":"Most HPC systems use a software module system like Lmod or Environment Modules to provide you with a convenient way to dynamically load pre-installed software applications, libraries, and other packages into your shell's environment. If you need to module load any software to create the environment for your Jupyter notebook session, you can do so by including them as a comma-separated list to the --env-modules option in your launch command. Each module included in the list will be loaded prior to starting jupyter . In some cases, the --env-modules command-line option may be the only one you need to define your software environment. For example, if you have a standard Python-based data science workflow that you want run on Expanse, then you might only need to load one of the Anaconda distributions available in its software module environment. galyleo launch --account abc123 --partition shared --cpus 2 --memory 4 --time-limit 00:30:00 --env-modules cpu/0.17.3b,anaconda3/2021.05 By default, each Anaconda distribution comes with over 250 of the most popular data science software packages pre-installed, including jupyter .","title":"Environment modules"},{"location":"galyleo/#singularity-containers","text":"Singularity containers bring operating system-level virtualization to scientific and high-performance computing, allowing you to package complete software environments --- including operating systems, software applications, libraries, and data --- in a simple, portable, and reproducible way, which can then be executed and run almost anywhere. If you have a Singularity container that you would like to run your Jupyter notebook session within, then you simply need to provide a path to the container with the --sif option in your launch command. This will start jupyter within the container using the singularity exec command. If necessary, you can also pass user-defined --bind mounts to the container and enable NVIDIA GPU support via the --nv flag. One of the most powerful features of Singularity is its ability to convert an existing Docker container to a Singularity container. So, even if you are not familiar with how to build your own Singularity container, you can always search public container registries like Docker Hub for an existing container that may help you get your work done. For example, let's say you need an R environment for your Jupyter notebook session. Why not try the latest r-notebook container from the Jupyter Docker Stacks project? To get started, you first use the singularity pull command to download and convert the Docker container to a Singularity container. singularity pull docker://jupyter/r-notebook:latest Once all of the layers of the Docker container have been downloaded and the container conversion process is complete, you can then launch your Jupyter notebook session with the newly built Singularity container. galyleo launch --account abc123 --cpus 2 --memory 4 --time-limit 00:30:00 --sif r-notebook_latest.sif On some systems like Expanse, you may need to load Singularity via the software module environment as well. galyleo launch --account abc123 --cpus 2 --memory 4 --time-limit 00:30:00 --env-modules singularitypro --sif r-notebook_latest.sif --bind /expanse,/scratch Here, the user-defined --bind mount option also enables access to both the /expanse network filesystems (e.g., /expanse/lustre ) and the local NVMe /scratch disk(s) available on each compute node from within the container. By default, only your $HOME directory is accessible from within the container. Singularity also provides native support for running containerized applications on NVIDIA GPUs. If you have a GPU-accelerated application you would like to run during your Jupyter notebook session, please make sure your container includes a CUDA-enabled version of the application that can utilize NVIDIA GPUs. NVIDIA distributes a number of GPU-optimized containers via their container registry . This includes containers for all of the most popular deep learning frameworks --- PyTorch , TensorFlow , and MXNet --- with jupyter pre-installed. Like the the containers available from DockerHub, you can pull these containers to the HPC system you are working on singularity pull docker://nvcr.io/nvidia/pytorch:21.07-py3 and then launch your Jupyter notebook session with galyleo . For example, you might want to run this PyTorch container on a single NVIDIA V100 GPU available in Expanse's gpu-shared partition. galyleo launch --account abc123 --partition gpu-shared --cpus 10 --memory 93 --gpus 1 --time-limit 00:30:00 --env-modules singularitypro --sif pytorch_21.07-py3.sif --bind /expanse,/scratch --nv Note, however, how you request GPU resources with galyleo may be different from one HPC system to another. For example, on Comet you must use the --gres command-line option on Comet to specify both the type and number of GPUs required for your Jupyter notebook session. The following launch command would create a session within the NVIDIA PyTorch container on a single P100 GPU available in Comet's gpu-shared partition. galyleo launch --account abc123 --partition gpu-shared --cpus 7 --gres gpu:p100:1 --time-limit 00:30:00 --sif pytorch_21.07-py3.sif --bind /oasis,/scratch --nv In contrast, on TSCC you'll never explicitly request a specific number of GPUs for your Jupyter notebook session. All GPUs on TSCC are currently allocated implicitly in proportion to the number of CPU-cores requested by a job and available on the type of GPU-accelerated compute node you expect it to run on. And if you would like to request your notebook session be scheduled on a certain type of GPU, then you must pass the type of GPU required listed in the pbsnodes properties via the --constraint command-line option. For example, the following launch command will schedule your session on one of the NVIDIA GeForce RTX ] 2080Ti GPUs available in the gpu-hotel queue on TSCC. galyleo launch --account abc123 --partition gpu-hotel --cpus 2 --constraint gpu2080ti --time-limit 00:30:00 --sif pytorch_21.07-py3.sif --bind /oasis --nv Whatever you do, whenever you're launching your Jupyter notebook session with galyleo from a Singularity container on compute resources with NVIDIA GPUs, don't forget the include the --nv flag.","title":"Singularity containers"},{"location":"galyleo/#conda-environments","text":"Conda is an open-source software package and environment manager developed by Anaconda Inc. . Its ease of use, compatibility across multiple operating systems, and comprehensive support for both the Python and R software ecosystems has made it one of the most popular ways to build and maintain custom software environments in the data science and machine learning communities. And because of the constantly evolving software landscape in these spaces, which can involve quite complex software dependencies, conda is often the simplest way to get your custom Python or R software environment up and running on an HPC system. galyleo supports the use of conda environments to configure the software environment for your Jupyter notebook session. If you've already installed a conda distribution --- we recommend Miniconda --- and configured a custom conda environment within it, then you should only need to specify the name of the conda environment you want to activate for your notebook session with the --conda-env command-line option. For example, let's imagine you've already created a custom conda environment from the following environment.yml file. name: notebooks-sharing channels: - conda-forge - anaconda dependencies: - python=3.7 - jupyterlab=3 - pandas=1.2.4 - matplotlib=3.4.2 - seaborn=0.11.0 - scikit-learn=0.23.2 You should then be able to launch a 30-minute JupyterLab session on a four CPU-cores with 8 GB of memory on one of Expanse's shared AMD compute nodes by simply activating the notebooks-sharing environment. galyleo launch --account abc123 --partition shared --cpus 4 --memory 8 --time-limit 00:30:00 --conda-env notebooks-sharing Note, however, the use of the --conda-env command-line option here assumes you've already configured your ~/.bashrc file with the conda init command. If you have not done so (or choose not to do so), then you can also initialize any conda distribution in your launch command by providing the path to its conda.sh initialization script in the etc/profile.d directory via the --conda-init command-line option. galyleo launch --account abc123 --partition shared --cpus 4 --memory 8 --time-limit 00:30:00 --conda-env notebooks-sharing --conda-init miniconda3/etc/profile.d/conda.sh While creating your own custom software environment with conda may be convenient, it can also generate a high metadata load on the types of shared network filesystems you'll often find on an HPC system. At a minimum, if you install your conda distribution on a network filesystem, you can expect this to increase the installation time of software packages into your conda environment when compared to a local filesystem installation you may have done previously on your laptop. Under some circumstances, this metadata issue can lead to a serious degradation of the aggregate I/O performance across a filesystem, affecting the performance of all user jobs on the system. If you have not yet installed your conda environment on a shared filesystem (such as in your $HOME directory), galyleo now also allows you to dynamically create the environment at runtime from an environment.yml file. To use this feature, you simply need to provide the name of the environment.yml file with the --conda-yml command-line option. For example, if you wanted to start an Juoyter notebook session with the notebooks-sharing environment, you would use the following command: galyleo launch --account abc123 --partition shared --cpus 4 --memory 8 --time-limit 00:30:00 --conda-env notebooks-sharing --conda-yml environment.yml You can further improve the installation performance and reuse of these dynamically generated conda environments by using the new --mamba and --cache command-line options, which enables the use of Mamba to speed up software installs and saves the completed conda environment using conda-pack for future reuse, respectively. galyleo launch --account abc123 --partition shared --cpus 4 --memory 8 --time-limit 00:30:00 --conda-env notebooks-sharing --conda-yml environment.yml --mamba --cache","title":"Conda environments"},{"location":"galyleo/#debugging-your-session","text":"If you experience a problem launching your Jupyter notebook session with galyleo , you may be able to debug the issue yourself by reviewing the batch job script generated by galyleo or the standard output/error file generated by the job itself. You can find these files stored in the hidden ~/.galyleo directory created in your HOME directory.","title":"Debugging your session"},{"location":"galyleo/#additional-information","text":"","title":"Additional Information"},{"location":"galyleo/#expanse-user-portal","text":"galyleo has been integrated with the Open OnDemand-based Expanse User Portal to help simplify launching Jupyter notebooks on Expanse. After logging into the portal, you can access this web-based interface to galyleo from the Interactive Apps tab in the toolbar across the top of your browser, then select Jupyter .","title":"Expanse User Portal"},{"location":"galyleo/#containers","text":"SDSC builds and maintains a number of custom Singularity containers for use on its HPC systems . Pre-built copies of many of these containers are made available from a central storage location on each HPC system. Please check the following locations for the latest containers. If you do not find the container you're looking for, please feel free to contact us and make a request for a container to be made available. On Expanse: - /cm/shared/apps/containers/singularity","title":"Containers"},{"location":"galyleo/#status","text":"A work in progress.","title":"Status"},{"location":"galyleo/#contribute","text":"If you would like to contribute to the project, then please submit a pull request via GitHub. If you have a feature request or a problem to report, then please create a GitHub issue.","title":"Contribute"},{"location":"galyleo/#author","text":"Marty Kandes, Ph.D. Computational & Data Science Research Specialist High-Performance Computing User Services Group Data-Enabled Scientific Computing Division San Diego Supercomputer Center University of California, San Diego","title":"Author"},{"location":"galyleo/#version","text":"0.7.6","title":"Version"},{"location":"galyleo/#last-updated","text":"Monday, May 6th, 2024","title":"Last Updated"},{"location":"hello_icicle_auth_clients/","text":"Hello ICICLE Authentication Clients Repo for Authenticated Clients and Applications for ICICLE CI Services Contents: * Overview * Software Releases * About the Team * Acknowledgement Overview The Artificial Intelligence (AI) institute for Intelligent Cyberinfrastructure with Computational Learning in the Environment (ICICLE) is funded by the NSF to build the next generation of Cyberinfrastructure to render AI more accessible to everyone and drive its further democratization in the larger society. ICICLE aims to develop intelligent cyberinfrastructure with transparent and high-performance execution on diverse and heterogeneous environments as well as advance plug-and-play AI that is easy to use by scientists across a wide range of domains, promoting the democratization of AI. The deep AI infrastructure that ICICLE plans to utilize to sift through data relies on knowledge graphs (KGs) in which information is stored in a graph database that uses a graph-structured data model or data model to represent a network of entities and the relationships between them. Finding a way to make KGs easily accessible and functional on high performance computing (HPC) systems is an important step in helping to democratize HPC. Thus a large focus of this project was contributing to the body of knowledge needed for hosting live, dynamic, and interactive services that interface with HPC systems hosting KGs for ICICLE based resources and services In this project, we develop Jupyter Notebooks and Python command line clients that will access ICICLE resources and services using ICICLE authentication mechanisms. To connect our clients, we used Tapis, which is a framework that supports computational research to enable scientists to access and utilize and manage multi-institution resources and services. We used Neo4j to organize data into a knowledge graph (KG). We then hosted the KG on a Tapis Pod, which offers persistent data storage with a template made specifically for Neo4j KGs. Software Releases icicle_rel_03_2023 For this software release, we focussed on developing authenticated connections to kubernetes pods hosted on any Tapis service. These applciations are stand-alone and can be installed separately. For details, see: * Jupyter Notebooks * Command Line Interfaces Command Line Applications Our CLI's are production software intended for use as interfaces to Tapis services hosted on HPC systems. These are ready to install and use, provided the proper requirements are fulfilled. Jupyter Notebooks The Jupyter notebooks in this repository are primarily demonstrators for working Tapis code, written in python. We also made an extensible template notebook which has Tapis auth prebuilt, and can be easily modified to carry out specific Tapis related tasks. The Notebooks and the CLIs each have their own directory and software requirements which are described here: About the Team This software was developed as part of the SDSC/UCSD Smmer 2022 REHS Project , titled Developing Interactive Jupyter Notebooks to run on the SDSC HPC Expanse System and the \u201cAI institute for Intelligent Cyberinfrastructure with Computational Learning in the Environment\u201d (ICICLE) project. Project Lead: Mary Thomas, Ph.D., SDSC HPC Training lead, and Computational Data Scientist in the Data-Enabled Scientific Computing Division. REHS Students: Sahil Samar, Del Norte High School, San Diego, CA, sahilsamar031@gmail.com Mia Chen, Westview High School, San Diego, CA, mialunachen@gmail.com Jack Karpinski, San Diego High School, San Diego, CA, USA, jackadoo4@gmail.com Michael Ray, JSerra Catholic High School, San Juan Capistrano, CA, michael.ray@jserra.org Archita Sarin, Mission San Jose High School, Fremont, CA, archita.sarin@gmail.com Collaborators/Mentors: Christian Garcia, Engineering Scientist Associate (Texas Advanced Computing Center [5]). Matthew Lange, Ph.D., CEO, International Center for Food Ontology Operability Data and Semantics (IC-FOODS [4]); Joe Stubbs, Ph.D., Manager, Cloud & Interactive Computing (Texas Advanced Computing Center [5]). Acknowledgement This work has been funded by grants from the National Science Foundation, including: * The AI Institute for Intelligent CyberInfrastructure with Computational Learning in the Environment (ICICLE) ( #2112606 ) * The SDSC Expanse project ( #1928224 ) * The TACC Stampede System ( #1663578 ) * Tapis projects ( #1931439 ) * the NSF Track 3 Award: COre National Ecosystem for CyberinfrasTructure (CONECT) ( #2138307 ) and the Extreme Science and Engineering Discovery Environment (XSEDE) ( ACI-1548562 ). NOTES: YAML file: See Component Data Yaml file: https://github.com/ICICLE-ai/CI-Components-Catalog/blob/master/components-data.yaml","title":"Hello ICICLE Authentication Clients"},{"location":"hello_icicle_auth_clients/#hello-icicle-authentication-clients","text":"Repo for Authenticated Clients and Applications for ICICLE CI Services Contents: * Overview * Software Releases * About the Team * Acknowledgement","title":"Hello ICICLE Authentication Clients"},{"location":"hello_icicle_auth_clients/#overview","text":"The Artificial Intelligence (AI) institute for Intelligent Cyberinfrastructure with Computational Learning in the Environment (ICICLE) is funded by the NSF to build the next generation of Cyberinfrastructure to render AI more accessible to everyone and drive its further democratization in the larger society. ICICLE aims to develop intelligent cyberinfrastructure with transparent and high-performance execution on diverse and heterogeneous environments as well as advance plug-and-play AI that is easy to use by scientists across a wide range of domains, promoting the democratization of AI. The deep AI infrastructure that ICICLE plans to utilize to sift through data relies on knowledge graphs (KGs) in which information is stored in a graph database that uses a graph-structured data model or data model to represent a network of entities and the relationships between them. Finding a way to make KGs easily accessible and functional on high performance computing (HPC) systems is an important step in helping to democratize HPC. Thus a large focus of this project was contributing to the body of knowledge needed for hosting live, dynamic, and interactive services that interface with HPC systems hosting KGs for ICICLE based resources and services In this project, we develop Jupyter Notebooks and Python command line clients that will access ICICLE resources and services using ICICLE authentication mechanisms. To connect our clients, we used Tapis, which is a framework that supports computational research to enable scientists to access and utilize and manage multi-institution resources and services. We used Neo4j to organize data into a knowledge graph (KG). We then hosted the KG on a Tapis Pod, which offers persistent data storage with a template made specifically for Neo4j KGs.","title":"Overview"},{"location":"hello_icicle_auth_clients/#software-releases","text":"","title":"Software Releases"},{"location":"hello_icicle_auth_clients/#icicle_rel_03_2023","text":"For this software release, we focussed on developing authenticated connections to kubernetes pods hosted on any Tapis service. These applciations are stand-alone and can be installed separately. For details, see: * Jupyter Notebooks * Command Line Interfaces","title":"icicle_rel_03_2023"},{"location":"hello_icicle_auth_clients/#command-line-applications","text":"Our CLI's are production software intended for use as interfaces to Tapis services hosted on HPC systems. These are ready to install and use, provided the proper requirements are fulfilled.","title":"Command Line Applications"},{"location":"hello_icicle_auth_clients/#jupyter-notebooks","text":"The Jupyter notebooks in this repository are primarily demonstrators for working Tapis code, written in python. We also made an extensible template notebook which has Tapis auth prebuilt, and can be easily modified to carry out specific Tapis related tasks. The Notebooks and the CLIs each have their own directory and software requirements which are described here:","title":"Jupyter Notebooks"},{"location":"hello_icicle_auth_clients/#about-the-team","text":"This software was developed as part of the SDSC/UCSD Smmer 2022 REHS Project , titled Developing Interactive Jupyter Notebooks to run on the SDSC HPC Expanse System and the \u201cAI institute for Intelligent Cyberinfrastructure with Computational Learning in the Environment\u201d (ICICLE) project. Project Lead: Mary Thomas, Ph.D., SDSC HPC Training lead, and Computational Data Scientist in the Data-Enabled Scientific Computing Division. REHS Students: Sahil Samar, Del Norte High School, San Diego, CA, sahilsamar031@gmail.com Mia Chen, Westview High School, San Diego, CA, mialunachen@gmail.com Jack Karpinski, San Diego High School, San Diego, CA, USA, jackadoo4@gmail.com Michael Ray, JSerra Catholic High School, San Juan Capistrano, CA, michael.ray@jserra.org Archita Sarin, Mission San Jose High School, Fremont, CA, archita.sarin@gmail.com Collaborators/Mentors: Christian Garcia, Engineering Scientist Associate (Texas Advanced Computing Center [5]). Matthew Lange, Ph.D., CEO, International Center for Food Ontology Operability Data and Semantics (IC-FOODS [4]); Joe Stubbs, Ph.D., Manager, Cloud & Interactive Computing (Texas Advanced Computing Center [5]).","title":"About the Team"},{"location":"hello_icicle_auth_clients/#acknowledgement","text":"This work has been funded by grants from the National Science Foundation, including: * The AI Institute for Intelligent CyberInfrastructure with Computational Learning in the Environment (ICICLE) ( #2112606 ) * The SDSC Expanse project ( #1928224 ) * The TACC Stampede System ( #1663578 ) * Tapis projects ( #1931439 ) * the NSF Track 3 Award: COre National Ecosystem for CyberinfrasTructure (CONECT) ( #2138307 ) and the Extreme Science and Engineering Discovery Environment (XSEDE) ( ACI-1548562 ). NOTES: YAML file: See Component Data Yaml file: https://github.com/ICICLE-ai/CI-Components-Catalog/blob/master/components-data.yaml","title":"Acknowledgement"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/","text":"Hello ICICLE Authentication Clients Software Release Notes: Software release: icicle_rel_03_2023 Date: 04/14/2023 For this software release, we focussed on developing authenticated connections to kubernetes pods hosted on a TACC server (URL???). The primary goals of these applications included: Authenticating users using TACC accounts Creating Neo4j Pods to be able to store data on Tacc Servers Loading pre-existing data into the Neo4j Pods Setting permissions of Pods Parsing Cypher queries from our clients and directly communicate with the data on our Pods Visualizing data through neo4jupyter (notebooks) Developing an interactive user interface for requesting Cypher input (CLIs) icicle_rel_03_2023 For this software release, we focussed on developing authenticated connections to kubernetes pods hosted on any Tapis service. These applciations are stand-alone and can be installed separately. For details, see: Jupyter Notebooks TapisAuth Example Applications Command Line Interfaces ICICONSOLE TapisCL-ICICLE The Notebooks and the CLIs each have their own directory and software requirements which are described here:","title":"Hello ICICLE Authentication Clients Software Release Notes:"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/#hello-icicle-authentication-clients-software-release-notes","text":"Software release: icicle_rel_03_2023 Date: 04/14/2023 For this software release, we focussed on developing authenticated connections to kubernetes pods hosted on a TACC server (URL???). The primary goals of these applications included: Authenticating users using TACC accounts Creating Neo4j Pods to be able to store data on Tacc Servers Loading pre-existing data into the Neo4j Pods Setting permissions of Pods Parsing Cypher queries from our clients and directly communicate with the data on our Pods Visualizing data through neo4jupyter (notebooks) Developing an interactive user interface for requesting Cypher input (CLIs)","title":"Hello ICICLE Authentication Clients Software Release Notes:"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/#icicle_rel_03_2023","text":"For this software release, we focussed on developing authenticated connections to kubernetes pods hosted on any Tapis service. These applciations are stand-alone and can be installed separately. For details, see: Jupyter Notebooks TapisAuth Example Applications Command Line Interfaces ICICONSOLE TapisCL-ICICLE The Notebooks and the CLIs each have their own directory and software requirements which are described here:","title":"icicle_rel_03_2023"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/","text":"Hello Icicle Authentication Clients Software Release Notes: Command Line Interface applications Software release: icicle_rel_03_2023 Date: 04/14/2023 Overview These two command line applications provide a user friendly way to access and manage Tapis services. General Requirements An account and login credentials for a Tapis service running on HPC system (TACC or Expanse for instance) A python installation TapisCL-ICICLE Functions: * Command line interface to manage and operate Tapis services * Pods * Services * Files * Apps/Jobs Dependencies: here Installation Options: 1. As PyPi package 2. Directly from github see here for installation instructions ICICONSOLE Functions: * Command line interface specifically aimed toward working with Neo4j hosted on Tapis pods. * Direct entry of cypher commands into the console to be executed by Neo4j * Also provides python library which wraps over Cypher commands Dependencies: here Installation Options: 1. As PyPi package 2. Directly from github see here for installation instructions","title":"Hello Icicle Authentication Clients Software Release Notes:"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/#hello-icicle-authentication-clients-software-release-notes","text":"","title":"Hello Icicle Authentication Clients Software Release Notes:"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/#command-line-interface-applications","text":"Software release: icicle_rel_03_2023 Date: 04/14/2023","title":"Command Line Interface applications"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/#overview","text":"These two command line applications provide a user friendly way to access and manage Tapis services.","title":"Overview"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/#general-requirements","text":"An account and login credentials for a Tapis service running on HPC system (TACC or Expanse for instance) A python installation","title":"General Requirements"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/#tapiscl-icicle","text":"Functions: * Command line interface to manage and operate Tapis services * Pods * Services * Files * Apps/Jobs Dependencies: here Installation Options: 1. As PyPi package 2. Directly from github see here for installation instructions","title":"TapisCL-ICICLE"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/#iciconsole","text":"Functions: * Command line interface specifically aimed toward working with Neo4j hosted on Tapis pods. * Direct entry of cypher commands into the console to be executed by Neo4j * Also provides python library which wraps over Cypher commands Dependencies: here Installation Options: 1. As PyPi package 2. Directly from github see here for installation instructions","title":"ICICONSOLE"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/ICICONSOLE/","text":"ICICONSOLE Overview ICICONSOLE is designed to provide an efficient and powerful interface to Neo4j Knowledge Graph databases hosted on HPC resources, leveraging Tapis. This application is specialized for knowledge graph querying, and has some basic CYPHER commands built in. Installation Right now, the only way to use this application is to directly run the python code. Additionally, it requies python 3.10. git clone https://github.com/icicle-ai/hello_icicle_auth_clients.git cd hello_icicle_auth_clients/icicle_rel_03_2023/CLI/ICICONSOLE Install dependencies. pip install pandas pip install py2neo pip install tapipy Run ICICONSOLE. python ICICONSOLE.py First time user guide You will be asked to login with your TACC account. If you aren't sure if you have this, visit the TACC portal . Next, you will see the Tapis Pods that you have been given permission to access. If you don't see any, please contact the owner of the Pod you wish to access. Type in the ID of the Pod that you want to access. Once you do this, you will be in a custom made console for interfacing with the Knowledge Graph, using the Cypher language. If you know Cypher, you can start typing in commands like MATCH(n) RETURN n LIMIT 10 If you are not familiar with Cypher, don't worry! This is meant for users who have never used Cypher before. Type in \"help\" to view some of the built in commands to start exploring the knowledge graph. These built in commands will grow more extensive as time goes on. The welcome message for the Knowledge Graph console contains helpful tips, like \"new\", \"exit\", \"clear\", and \"help\".","title":"ICICONSOLE"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/ICICONSOLE/#iciconsole","text":"","title":"ICICONSOLE"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/ICICONSOLE/#overview","text":"ICICONSOLE is designed to provide an efficient and powerful interface to Neo4j Knowledge Graph databases hosted on HPC resources, leveraging Tapis. This application is specialized for knowledge graph querying, and has some basic CYPHER commands built in.","title":"Overview"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/ICICONSOLE/#installation","text":"Right now, the only way to use this application is to directly run the python code. Additionally, it requies python 3.10. git clone https://github.com/icicle-ai/hello_icicle_auth_clients.git cd hello_icicle_auth_clients/icicle_rel_03_2023/CLI/ICICONSOLE Install dependencies. pip install pandas pip install py2neo pip install tapipy Run ICICONSOLE. python ICICONSOLE.py","title":"Installation"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/ICICONSOLE/#first-time-user-guide","text":"You will be asked to login with your TACC account. If you aren't sure if you have this, visit the TACC portal . Next, you will see the Tapis Pods that you have been given permission to access. If you don't see any, please contact the owner of the Pod you wish to access. Type in the ID of the Pod that you want to access. Once you do this, you will be in a custom made console for interfacing with the Knowledge Graph, using the Cypher language. If you know Cypher, you can start typing in commands like MATCH(n) RETURN n LIMIT 10 If you are not familiar with Cypher, don't worry! This is meant for users who have never used Cypher before. Type in \"help\" to view some of the built in commands to start exploring the knowledge graph. These built in commands will grow more extensive as time goes on. The welcome message for the Knowledge Graph console contains helpful tips, like \"new\", \"exit\", \"clear\", and \"help\".","title":"First time user guide"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/","text":"TapisCLI Please remember to create an issue in this repository if you encounter any bugs, we will do our best to fix it quick! Overview Tapis CLI is designed to provide a simple to use, versatile way to interface with Tapis services hosted on HPC resources. User can either start the app and use it as a traditional command line applications, or pass commands directly from bash. Allows you to work with all major Tapis services: Pods, Systems, Files, and Apps in one place. It can also interface directly with services being hosted on Tapis pods, like Neo4j. Although currently Neo4j is the only 3rd party application it can work with, adding support for Postgres and the like will not be difficult. Dependencies Dependencies are listed here Installation Using PyPi pip install TapisCL-ICICLE . Current version 0.0.24 python -m TapisCLICICLE Running Python Code Directly Clone the repository to local machine. python -m pip install -r requirements.txt python cli.py Operations Full Terminal Interface: 1. run python -m TapisCLICICLE 2. You will be promted to enter a Tapis service link. You can find this on the Tapis service provider's wesbite usually. If you are working with icicle, this should be https://icicle.tapis.io 3. enter your username and password when prompted 4. if all went well the console should open. You can run help to see command options 5. to exit the application, run exit Command Line: Alternatively, if you do not want to enter the actual command line environment of the TapisCL-ICICLE application, you can run commands directly from the command line like this: python -m TapisCLICICLE pods -c help this may still ask you for authentication, however once you are logged in once, you do not need to enter your credentials again unless the 5 minute timeout period passes, in which case the application shuts itself off. Scripting Python and Bash scripting examples are available here","title":"TapisCLI"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/#tapiscli","text":"Please remember to create an issue in this repository if you encounter any bugs, we will do our best to fix it quick!","title":"TapisCLI"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/#overview","text":"Tapis CLI is designed to provide a simple to use, versatile way to interface with Tapis services hosted on HPC resources. User can either start the app and use it as a traditional command line applications, or pass commands directly from bash. Allows you to work with all major Tapis services: Pods, Systems, Files, and Apps in one place. It can also interface directly with services being hosted on Tapis pods, like Neo4j. Although currently Neo4j is the only 3rd party application it can work with, adding support for Postgres and the like will not be difficult.","title":"Overview"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/#dependencies","text":"Dependencies are listed here","title":"Dependencies"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/#installation","text":"","title":"Installation"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/#using-pypi","text":"pip install TapisCL-ICICLE . Current version 0.0.24 python -m TapisCLICICLE","title":"Using PyPi"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/#running-python-code-directly","text":"Clone the repository to local machine. python -m pip install -r requirements.txt python cli.py","title":"Running Python Code Directly"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/#operations","text":"Full Terminal Interface: 1. run python -m TapisCLICICLE 2. You will be promted to enter a Tapis service link. You can find this on the Tapis service provider's wesbite usually. If you are working with icicle, this should be https://icicle.tapis.io 3. enter your username and password when prompted 4. if all went well the console should open. You can run help to see command options 5. to exit the application, run exit Command Line: Alternatively, if you do not want to enter the actual command line environment of the TapisCL-ICICLE application, you can run commands directly from the command line like this: python -m TapisCLICICLE pods -c help this may still ask you for authentication, however once you are logged in once, you do not need to enter your credentials again unless the 5 minute timeout period passes, in which case the application shuts itself off. Scripting Python and Bash scripting examples are available here","title":"Operations"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/Scripting-Examples/","text":"Scripting examples Overview TapisCLICICLE's can be scripted to automatically execute tasks for managing Tapis services. You can do this using: Bash Script: execute a collection of TapisCLICICLE commands from one file Jupyter Notebook/Python: By importing the TapisCLICICLE's constituent packages, most notably the tapisObjectWrappers (which governs the CLI interface with the Tapis API through the TACC Tapis python library, and adds additional function to streamline operations), you can also write python scripts to automate CLI functions.","title":"Scripting examples"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/Scripting-Examples/#scripting-examples","text":"","title":"Scripting examples"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/Scripting-Examples/#overview","text":"TapisCLICICLE's can be scripted to automatically execute tasks for managing Tapis services. You can do this using: Bash Script: execute a collection of TapisCLICICLE commands from one file Jupyter Notebook/Python: By importing the TapisCLICICLE's constituent packages, most notably the tapisObjectWrappers (which governs the CLI interface with the Tapis API through the TACC Tapis python library, and adds additional function to streamline operations), you can also write python scripts to automate CLI functions.","title":"Overview"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/TapisCLICICLE/","text":"For those interested Design TapisCLICICLE uses a localhost client server model to run the application. While this did introduce some complexites to the code, it has several benefits. 1. The Tapis objects, and all other initialization only needs to happen once when the server is turned on. Initialization takes both time, and resources 2. The user only has to log in once for a 5 minute extendable session, so if they want to run commands directly from a bash environment they do not have to log in for every command 3. The program doesnt have to continuously make new login requests to the Tapis service they are connecting to","title":"Index"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/TapisCLICICLE/#for-those-interested","text":"","title":"For those interested"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/TapisCLICICLE/#_1","text":"","title":""},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/TapisCLICICLE/#design","text":"TapisCLICICLE uses a localhost client server model to run the application. While this did introduce some complexites to the code, it has several benefits. 1. The Tapis objects, and all other initialization only needs to happen once when the server is turned on. Initialization takes both time, and resources 2. The user only has to log in once for a 5 minute extendable session, so if they want to run commands directly from a bash environment they do not have to log in for every command 3. The program doesnt have to continuously make new login requests to the Tapis service they are connecting to","title":"Design"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/tapis-config-files/","text":"Config Files This is where examples for config files go. In order to make full use of the client, the user must load config files for systems and apps, etc.","title":"Config Files"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/CLI/TapisCL-ICICLE/tapis-config-files/#config-files","text":"This is where examples for config files go. In order to make full use of the client, the user must load config files for systems and apps, etc.","title":"Config Files"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/Notebooks/","text":"Hello Icicle Authentication Clients Software Release Notes: Secure Notebooks for Accessing Tapis PODs Software release: icicle_rel_03_2023 Date: 04/14/2023 Notebook Overview These notebooks are used as templates for developing Tapis authenticated applications, or as demonstrators/examples of what you can do with Tapis in your applications. TapisAuthTemplate This notebook provides an extensible, easy to use base to build out a Tapis authenticated application. It comes with the Tapis auth included, and allows the user to add code to interface with Tapis without having to worry about the authorization. requirements: here these apply to the other applications below as well Example Notebooks Demonstrators of possible Tapis use cases Pods These notebooks are demonstrators for the Tapis pods service, for now focusing on Neo4j pods which can host knowledge graphs. They also demonstrate usage of the extensible Tapis Auth notebook. load_data Demonstrates several methods of loading data from CSV files into a Neo4j knowledge graph, through Tapis. PPod Demonstrates querying of a Neo4j knowledge graph pod hosting the ICICLE PPod ontology. query_neo4j_architecture Demonstrates querying of a Neo4j knowledge graph pod hosting the Tapis architecture diagram","title":"Hello Icicle Authentication Clients Software Release Notes:"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/Notebooks/#hello-icicle-authentication-clients-software-release-notes","text":"Secure Notebooks for Accessing Tapis PODs Software release: icicle_rel_03_2023 Date: 04/14/2023","title":"Hello Icicle Authentication Clients Software Release Notes:"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/Notebooks/#notebook-overview","text":"These notebooks are used as templates for developing Tapis authenticated applications, or as demonstrators/examples of what you can do with Tapis in your applications.","title":"Notebook Overview"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/Notebooks/#tapisauthtemplate","text":"This notebook provides an extensible, easy to use base to build out a Tapis authenticated application. It comes with the Tapis auth included, and allows the user to add code to interface with Tapis without having to worry about the authorization. requirements: here these apply to the other applications below as well","title":"TapisAuthTemplate"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/Notebooks/#example-notebooks","text":"Demonstrators of possible Tapis use cases","title":"Example Notebooks"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/Notebooks/#pods","text":"These notebooks are demonstrators for the Tapis pods service, for now focusing on Neo4j pods which can host knowledge graphs. They also demonstrate usage of the extensible Tapis Auth notebook.","title":"Pods"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/Notebooks/#load_data","text":"Demonstrates several methods of loading data from CSV files into a Neo4j knowledge graph, through Tapis.","title":"load_data"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/Notebooks/#ppod","text":"Demonstrates querying of a Neo4j knowledge graph pod hosting the ICICLE PPod ontology.","title":"PPod"},{"location":"hello_icicle_auth_clients/icicle_rel_03_2023/Notebooks/#query_neo4j_architecture","text":"Demonstrates querying of a Neo4j knowledge graph pod hosting the Tapis architecture diagram","title":"query_neo4j_architecture"},{"location":"hello_icicle_auth_clients/images/readme/","text":"images","title":"Readme"}]}